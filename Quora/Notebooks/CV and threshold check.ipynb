{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport re\nimport os\nimport pandas as pd\nimport random\nimport numpy as np\nfrom unidecode import unidecode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nimport string\nimport re\nimport math\nimport operator\nimport time\nfrom keras.models import Model, Sequential\nfrom keras import layers\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras import backend as K\nfrom keras import initializers, regularizers, constraints, optimizers\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5a187360988431a99abd0f653146743d0845b3f"},"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = '0'\n    np.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# GENERAL HYPERPARAMS\nnum_folds = 5\nseed = 42\n\n# HYPERPARAMS FOR TEXT PROCESSING\nmax_features = 200000\nmaxlen = 100\n\n# HYPERPARAMS FOR NN\nbatch_size = 1024\nepochs = 2\nembed_size = 300\n\nset_seed(seed)\n\n# PATH TO DATA DIRECTORY\nPATH = \"../input/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e44db4a743c3893ee324de2037dbb025e5312d02"},"cell_type":"code","source":"puncts = {',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√'}\n\ndef clean_text(x):\n    x = str(x)\n    table = str.maketrans({key: ' {punct} ' for key in puncts})\n    return x.translate(table)\n\ndef clean_numbers(x):\n    if bool(re.search(r'\\d', x)):\n        x = re.sub('[0-9]{5,}', '#####', x)\n        x = re.sub('[0-9]{4}', '####', x)\n        x = re.sub('[0-9]{3}', '###', x)\n        x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a03b7fff3daea714142a4af87c678adba7d97fb"},"cell_type":"code","source":"class CyclicLR(Callback):\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"125d55f48d7178a3b2893dc35bb900440c0e77af"},"cell_type":"markdown","source":"# Word Features"},{"metadata":{"trusted":true,"_uuid":"2ad39e1a28bdea0475ccc9cd3526295977565da2"},"cell_type":"code","source":"def legacy_round(number, points=0):\n    p = 10 ** points\n    return float(math.floor((number * p) + math.copysign(0.5, number))) / p\n\ndef char_count(text, ignore_spaces=True):\n        if ignore_spaces:\n            text = text.replace(\" \", \"\")\n        return len(text)\n\ndef lexicon_count(text):\n        count = len(text.split())\n        return count\n    \ndef syllable_count(text):\n        text = text.lower()\n        text = \"\".join(x for x in text if x not in list(string.punctuation))\n        if not text:\n            return 0\n        count = 0\n        vowels = 'aeiouy'\n        for word in text.split(' '):\n            word = word.strip(\".:;?!\")\n            if len(word) < 1:\n                continue\n            if word[0] in vowels:\n                count +=1\n            for index in range(1,len(word)):\n                if word[index] in vowels and word[index-1] not in vowels:\n                    count +=1\n            if word.endswith('e'):\n                count -= 1\n            if word.endswith('le'):\n                count+=1\n            if count == 0:\n                count +=1\n        return count\n\ndef sentence_count(text):\n        ignore_count = 0\n        sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]*[ |\\n](?=[A-Z])', text)\n        for sentence in sentences:\n            if lexicon_count(sentence) <= 2:\n                ignore_count += 1\n        return max(1, len(sentences) - ignore_count)\n    \ndef avg_sentence_length(text):\n        try:\n            asl = float(lexicon_count(text) / sentence_count(text))\n            return legacy_round(asl, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_syllables_per_word(text):\n        syllable = syllable_count(text)\n        words = lexicon_count(text)\n        try:\n            syllables_per_word = float(syllable) / float(words)\n            return legacy_round(syllables_per_word, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_letter_per_word(text):\n        try:\n            letters_per_word = float(\n                char_count(text) / lexicon_count(text))\n            return legacy_round(letters_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_sentence_per_word(text):\n        try:\n            sentence_per_word = float(\n                sentence_count(text) / lexicon_count(text))\n            return legacy_round(sentence_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n        \ndef flesch_reading_ease(text):\n        sentence_length = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = (\n            206.835\n            - float(1.015 * sentence_length)\n            - float(84.6 * syllables_per_word)\n        )\n        return legacy_round(flesch, 2)\n\ndef flesch_kincaid_grade(text):\n        sentence_lenth = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = (\n            float(0.39 * sentence_lenth)\n            + float(11.8 * syllables_per_word)\n            - 15.59)\n        return legacy_round(flesch, 1)\n\ndef polysyllabcount(text):\n        count = 0\n        for word in text.split():\n            wrds = syllable_count(word)\n            if wrds >= 3:\n                count += 1\n        return count\n\ndef smog_index(text):\n        sentences = sentence_count(text)\n        if sentences >= 3:\n            try:\n                poly_syllab = polysyllabcount(text)\n                smog = (\n                    (1.043 * (30 * (poly_syllab / sentences)) ** .5)\n                    + 3.1291)\n                return legacy_round(smog, 1)\n            except ZeroDivisionError:\n                return 0.0\n        else:\n            return 0.0\n\ndef coleman_liau_index(text):\n        letters = legacy_round(avg_letter_per_word(text)*100, 2)\n        sentences = legacy_round(avg_sentence_per_word(text)*100, 2)\n        coleman = float((0.058 * letters) - (0.296 * sentences) - 15.8)\n        return legacy_round(coleman, 2)\n\ndef automated_readability_index(text):\n        chrs = char_count(text)\n        words = lexicon_count(text)\n        sentences = sentence_count(text)\n        try:\n            a = float(chrs)/float(words)\n            b = float(words) / float(sentences)\n            readability = (\n                (4.71 * legacy_round(a, 2))\n                + (0.5 * legacy_round(b, 2))\n                - 21.43)\n            return legacy_round(readability, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef linsear_write_formula(text):\n        easy_word = 0\n        difficult_word = 0\n        text_list = text.split()[:100]\n        for word in text_list:\n            if syllable_count(word) < 3:\n                easy_word += 1\n            else:\n                difficult_word += 1\n        text = ' '.join(text_list)\n        number = float(\n            (easy_word * 1 + difficult_word * 3)\n            / sentence_count(text))\n        if number <= 20:\n            number -= 2\n        return number / 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24a29dc3c3569211c61af4f0b3544e44f4e88497"},"cell_type":"markdown","source":"# Metric"},{"metadata":{"trusted":true,"_uuid":"234e79ec68813fb257bf0327dd53d2f1514e4397"},"cell_type":"code","source":"def f1_score(true,pred):\n    #considering sigmoid activation, threshold = 0.5\n    pred = K.cast(K.greater(pred,0.5), K.floatx())\n\n    groundPositives = K.sum(true) + K.epsilon()\n    correctPositives = K.sum(true * pred) + K.epsilon()\n    predictedPositives = K.sum(pred) + K.epsilon()\n\n    precision = correctPositives / predictedPositives\n    recall = correctPositives / groundPositives\n\n    m = (2 * precision * recall) / (precision + recall)\n\n    return m","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbad601c591d2a37408bc81f2c590c861d7f80af"},"cell_type":"markdown","source":"# Misc Functions"},{"metadata":{"trusted":true,"_uuid":"c08d1e6069ba82e39ebc01bbe1058d1d71886201"},"cell_type":"code","source":"def clean_text_for_features(x):\n    special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0eb973446bf71f1448d5939caffcbeec4b853c5d"},"cell_type":"code","source":"def add_features(df, function_list):\n    df['question_text'] = df['question_text'].apply(lambda x:str(x))\n    for text_function in function_list:\n        df[text_function.__name__] = df['question_text'].apply(lambda x: text_function(str(x)))\n    df['total_length'] = df['question_text'].apply(len)\n    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/(float(row['total_length'])+1),\n                                axis=1)\n    df['num_words'] = df['question_text'].str.count('\\S+')\n    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / (df['num_words']+1)\n    del df['num_unique_words'], df['num_words'], df['capitals'], df['total_length']\n    gc.collect()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f61f55a2a7a246f0decd47f480bd39a584b995d"},"cell_type":"markdown","source":"# Model Architecture"},{"metadata":{"trusted":true,"_uuid":"7479be3fe1027dabaf0557a5188c4a122afa3b1a"},"cell_type":"code","source":"def dnn_model(features, embedding_weights):\n    features_input = layers.Input(shape=(features.shape[1],))\n    inp = layers.Input(shape=(maxlen, ))\n    x = layers.Embedding(embedding_weights.shape[0], embedding_weights.shape[1], weights=[embedding_weights], trainable=False)(inp)\n        \n    x = layers.Bidirectional(layers.CuDNNLSTM(64, kernel_initializer='glorot_normal', return_sequences = True))(x)\n    x, x_h, x_c = layers.Bidirectional(layers.CuDNNGRU(64, kernel_initializer='glorot_normal', return_sequences=True, return_state = True))(x)\n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    \n    x = layers.concatenate([avg_pool, x_h, max_pool, features_input])\n    x = layers.Dense(32, activation=\"tanh\", kernel_initializer='glorot_normal')(x)\n    x = layers.Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_normal')(x)\n    \n    model = Model(inputs=[inp,features_input], outputs=x)\n    adam = optimizers.adam(clipvalue=1.0)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=[f1_score])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe22c3a2a5371833eb80c81d0c3885c940d0fcdd"},"cell_type":"markdown","source":"# Text functions"},{"metadata":{"trusted":true,"_uuid":"c7a0386970ec33342c16bd792e5ec793e54f4d7e"},"cell_type":"code","source":"#text_function_list = [char_count, lexicon_count, syllable_count, sentence_count, avg_letter_per_word, avg_sentence_length, avg_sentence_per_word, avg_syllables_per_word, flesch_kincaid_grade, flesch_reading_ease, polysyllabcount, smog_index, coleman_liau_index, automated_readability_index, linsear_write_formula]\ntext_function_list = []","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a7b8d9d8012022864542b75514a7965696d86e5"},"cell_type":"markdown","source":"# Begin Main"},{"metadata":{"trusted":true,"_uuid":"fc9c5eae5f596e390afa677aa97818ec75b71ebe"},"cell_type":"code","source":"train_df = pd.read_csv(PATH+'train.csv', usecols=['question_text', 'target'])\ntest_df = pd.read_csv(PATH+'test.csv', usecols = ['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5388daf849c3cee10a37f47ff1322197d038935"},"cell_type":"code","source":"# lower\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n    \n# Clean the text\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n    \n# Clean numbers\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n    \n# Clean spellings\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"864b1688fc7d5dddb9930bb5f58924d90a17b9c9"},"cell_type":"code","source":"# FOR CREATING PROCESSED DATA AND LABELS\ntrain_sentences = train_df[\"question_text\"].fillna(\"_##_\").values\ntrain_labels = train_df['target']\ntest_sentences = test_df['question_text'].fillna(\"_##_\").values\n\ndel train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ba6ff3fd7eab7cb4e26c58b3a7a2c000f05601e"},"cell_type":"code","source":"train_df = pd.read_csv(PATH+'train.csv', usecols=['question_text', 'target'])\ntest_df = pd.read_csv(PATH+'test.csv', usecols = ['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96af576b34ccb3962b41fd63981a7689786325fe"},"cell_type":"code","source":"train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text_for_features(str(x)))\ntest_df['question_text'] = test_df['question_text'].apply(lambda x: clean_text_for_features(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ad627cc740a251260f91c55626319167f4efc2e"},"cell_type":"code","source":"# CREATE TEXT FEATURES\ntrain_df = add_features(train_df, text_function_list)\ntest_df = add_features(test_df, text_function_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba1ba515b4618853b4be9f62aca727de20f1b04e"},"cell_type":"code","source":"# SAVE AND PROCESS FEATURES TO SEND TO NN\ntrain_features = train_df.drop(['question_text', 'target'], axis=1)\ntest_features = test_df.drop(['question_text'], axis=1)\ndel train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1092cee1938c461e46afa5c1355ee57c509c0fc"},"cell_type":"code","source":"ss = StandardScaler()\nss.fit(np.vstack((train_features, test_features)))\ntrain_features = ss.transform(train_features)\ntest_features = ss.transform(test_features)\n\ndel ss\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83588f690240b42091da13f17fd6e425f7326fcb"},"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sentences) + list(test_sentences))\n    \ntokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n    \ntokenized_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)\n    \ndel tokenized_test, tokenized_train, train_sentences, test_sentences\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1729dff4475722768f36529a9927bde1a9018dac"},"cell_type":"code","source":"# FOLDS FOR CV\nfolds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9f641789b7c76a5b1d634ef7038d97d21f58fb9"},"cell_type":"code","source":"# LIST OF ALL EMBEDDINGS USED\nembedding_list = [PATH+'embeddings/paragram_300_sl999/paragram_300_sl999.txt', \nPATH+'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec',\nPATH+'embeddings/glove.840B.300d/glove.840B.300d.txt']\n    \n# TO SAVE FINAL PREDICTIONS\nfinal_preds = np.zeros((X_test.shape[0], 1))\noof_preds = np.zeros((train_features.shape[0],1))\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index)+1)\nout_embedding_matrix = np.zeros((nb_words, embed_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a21dfcc8757140e1c4b879c6931c8370648cdff"},"cell_type":"code","source":"emb_mean_dict = {'paragram_300_sl999':-0.005324783269315958,\n                'wiki-news-300d-1M':-0.0033469984773546457,\n                'glove.840B.300d':-0.005838498938828707}\n\nemb_std_dict = {'paragram_300_sl999':0.4934646189212799,\n                'wiki-news-300d-1M':0.10985549539327621,\n                'glove.840B.300d':0.4878219664096832}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff634f7e2bc22c7a795687e1f0fc689ebc96ff2d"},"cell_type":"code","source":"for EMBEDDING_FILE in embedding_list:\n        embedding_name = EMBEDDING_FILE.split('/')[3]\n        print('>>\\t CREATING EMBEDDINGS FOR {}!'.format(embedding_name))\n        emb_mean, emb_std = emb_mean_dict[embedding_name], emb_std_dict[embedding_name]\n        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n        for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore'):\n            word, vec = o.split(' ', 1)\n            if word not in word_index:\n                continue\n            i = word_index[word]\n            if i >= nb_words:\n                continue\n            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:embed_size]\n            if len(embedding_vector) == embed_size:\n                embedding_matrix[i] = embedding_vector\n        gc.collect()\n        print('>>\\t CREATING EMBEDDINGS FOR {} \\t DONE!'.format(embedding_name))\n        out_embedding_matrix += embedding_matrix\n        del embedding_matrix, embedding_vector\nglobal_embedding = out_embedding_matrix/3\ndel out_embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d9f23fcbf0dae20d9c94257f896691fd8ca7573"},"cell_type":"code","source":"for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_features, train_labels)):\n            print('FOLD NUMBER {}:'.format(n_fold+1))\n            train_x, train_feat, train_y = X_train[train_idx], train_features[train_idx], train_labels[train_idx]\n            valid_x, valid_feat, valid_y = X_train[valid_idx], train_features[valid_idx], train_labels[valid_idx]\n            \n            embedding_matrix = global_embedding.copy()\n            \n            # TRAIN ON FIXED EMBEDDINGS\n            print('>>\\t TRAINING FOR FIXED!')\n            model = dnn_model(train_feat, embedding_matrix)\n            model.fit([train_x, train_feat], train_y, batch_size=batch_size, epochs=epochs, shuffle = True)\n            print('>>\\t TRAINING EMBEDDINGS FOR FIXED DONE!')\n\n            # TRAIN ON TRAINABLE EMBEDDINGS\n            model.layers[1].trainable = True\n            adam = optimizers.adam(clipvalue=1.0)\n            model.compile(loss='binary_crossentropy',\n                              optimizer=adam,\n                              metrics=[f1_score])\n\n            print('>>\\t TRAINING EMBEDDINGS FOR TRAINABLE!')\n            model.fit([train_x, train_feat], train_y, batch_size=batch_size, epochs=epochs, shuffle = True)\n            print('>>\\t TRAINING EMBEDDINGS FOR TRAINABLE DONE!')\n\n            # PREDICT AND SAVE PREDICTIONS\n            print('>>\\t PREDICTING!')\n            valid_preds = model.predict([valid_x,valid_feat], batch_size=batch_size)\n            oof_preds[valid_idx] = valid_preds\n            final_preds += model.predict([X_test,test_features], batch_size=batch_size)\n            print('FOLD %2d AUC : %.6f' % (n_fold + 1, metrics.roc_auc_score(valid_y, valid_preds)))\n            print('>>\\t PREDICTING FOR DONE!')\n        \n            del train_x, train_y, valid_x, valid_y, train_feat, valid_feat, valid_preds, model, embedding_matrix\n            gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cb0f72542b728c46a109497c9a024375e32ad8d"},"cell_type":"code","source":"print('Overall AUC : %.6f' % (metrics.roc_auc_score(train_labels, oof_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bab49491680c6f3ea53326e3560b9559f536eca2"},"cell_type":"code","source":"final_preds = final_preds/5\nprint('>>\\t CREATING FINAL SUBMISSION FILE!')\nfinal_preds = (final_preds > 0.34).astype(int)\nsample = pd.read_csv(PATH+'sample_submission.csv')\nsample['prediction'] = final_preds\nsample.to_csv('submission.csv', index=False)\nprint('>>\\t CREATING FINAL SUBMISSION FILE \\t DONE!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d6bdddeea8e04f740caa04c0e3e1d407e60460f"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfcda7e2b4014c15aabc21a2a7478c0b7d84188c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}