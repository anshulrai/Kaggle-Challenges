{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import gc\nimport re\nimport os\nimport pandas as pd\nimport numpy as np\nfrom unidecode import unidecode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nimport string\nimport re\nimport math\nimport operator\nfrom pyphen import Pyphen\nimport time\nprint(os.listdir(\"../input\"))",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['train-test-result-analysis', 'quora-insincere-questions-classification']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a2b61717e468b5a6b9820240af45236260c7168"
      },
      "cell_type": "code",
      "source": "from keras.models import Model, Sequential\nfrom keras import layers\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras import backend as K\nfrom keras import optimizers",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3334066cad56e6bc24f7060499bb332183338a44",
        "_kg_hide-input": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "class CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "725ff94bf3d67b511e074f2dae88bfc2c18bfb64"
      },
      "cell_type": "code",
      "source": "exclude = list(string.punctuation)\ndic = Pyphen(lang='en_US')\n\ndef legacy_round(number, points=0):\n    p = 10 ** points\n    return float(math.floor((number * p) + math.copysign(0.5, number))) / p\n\ndef char_count(text, ignore_spaces=True):\n        if ignore_spaces:\n            text = text.replace(\" \", \"\")\n        return len(text)\n\ndef lexicon_count(text):\n        count = len(text.split())\n        return count\n    \ndef syllable_count(text):\n        text = text.lower()\n        text = \"\".join(x for x in text if x not in exclude)\n        if not text:\n            return 0\n        count = 0\n        vowels = 'aeiouy'\n        for word in text.split(' '):\n            #word_hyphenated = dic.inserted(word)\n            #count += max(1, word_hyphenated.count(\"-\") + 1)\n            word = word.strip(\".:;?!\")\n            if len(word) < 1:\n                continue\n            if word[0] in vowels:\n                count +=1\n            for index in range(1,len(word)):\n                if word[index] in vowels and word[index-1] not in vowels:\n                    count +=1\n            if word.endswith('e'):\n                count -= 1\n            if word.endswith('le'):\n                count+=1\n            if count == 0:\n                count +=1\n        return count\n\ndef sentence_count(text):\n        ignore_count = 0\n        sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]*[ |\\n](?=[A-Z])', text)\n        for sentence in sentences:\n            if lexicon_count(sentence) <= 2:\n                ignore_count += 1\n        return max(1, len(sentences) - ignore_count)\n    \ndef avg_sentence_length(text):\n        try:\n            asl = float(lexicon_count(text) / sentence_count(text))\n            return legacy_round(asl, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_syllables_per_word(text):\n        syllable = syllable_count(text)\n        words = lexicon_count(text)\n        try:\n            syllables_per_word = float(syllable) / float(words)\n            return legacy_round(syllables_per_word, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_letter_per_word(text):\n        try:\n            letters_per_word = float(\n                char_count(text) / lexicon_count(text))\n            return legacy_round(letters_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n\n\ndef avg_sentence_per_word(text):\n        try:\n            sentence_per_word = float(\n                sentence_count(text) / lexicon_count(text))\n            return legacy_round(sentence_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n        \ndef flesch_reading_ease(text):\n        sentence_length = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = (\n            206.835\n            - float(1.015 * sentence_length)\n            - float(84.6 * syllables_per_word)\n        )\n        return legacy_round(flesch, 2)\n\ndef flesch_kincaid_grade(text):\n        sentence_lenth = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = (\n            float(0.39 * sentence_lenth)\n            + float(11.8 * syllables_per_word)\n            - 15.59)\n        return legacy_round(flesch, 1)\n\ndef polysyllabcount(text):\n        count = 0\n        for word in text.split():\n            wrds = syllable_count(word)\n            if wrds >= 3:\n                count += 1\n        return count\n\ndef smog_index(text):\n        sentences = sentence_count(text)\n        if sentences >= 3:\n            try:\n                poly_syllab = polysyllabcount(text)\n                smog = (\n                    (1.043 * (30 * (poly_syllab / sentences)) ** .5)\n                    + 3.1291)\n                return legacy_round(smog, 1)\n            except ZeroDivisionError:\n                return 0.0\n        else:\n            return 0.0\n\ndef coleman_liau_index(text):\n        letters = legacy_round(avg_letter_per_word(text)*100, 2)\n        sentences = legacy_round(avg_sentence_per_word(text)*100, 2)\n        coleman = float((0.058 * letters) - (0.296 * sentences) - 15.8)\n        return legacy_round(coleman, 2)\n\ndef automated_readability_index(text):\n        chrs = char_count(text)\n        words = lexicon_count(text)\n        sentences = sentence_count(text)\n        try:\n            a = float(chrs)/float(words)\n            b = float(words) / float(sentences)\n            readability = (\n                (4.71 * legacy_round(a, 2))\n                + (0.5 * legacy_round(b, 2))\n                - 21.43)\n            return legacy_round(readability, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef linsear_write_formula(text):\n        easy_word = 0\n        difficult_word = 0\n        text_list = text.split()[:100]\n        for word in text_list:\n            if syllable_count(word) < 3:\n                easy_word += 1\n            else:\n                difficult_word += 1\n        text = ' '.join(text_list)\n        number = float(\n            (easy_word * 1 + difficult_word * 3)\n            / sentence_count(text))\n        if number <= 20:\n            number -= 2\n        return number / 2\n",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eceaf025e30675531476a38d69cd63ffd8210c53"
      },
      "cell_type": "code",
      "source": "text_function_list = [char_count, lexicon_count, syllable_count, sentence_count, avg_letter_per_word,\n                     avg_sentence_length, avg_sentence_per_word, avg_syllables_per_word, flesch_kincaid_grade, flesch_reading_ease,\n                     polysyllabcount, smog_index, coleman_liau_index, automated_readability_index, linsear_write_formula]",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "639b9ea1e17020b519446c944f3a59b501c6d64a"
      },
      "cell_type": "code",
      "source": "def f1_score(true,pred): #considering sigmoid activation, threshold = 0.5\n    pred = K.cast(K.greater(pred,0.5), K.floatx())\n\n    groundPositives = K.sum(true) + K.epsilon()\n    correctPositives = K.sum(true * pred) + K.epsilon()\n    predictedPositives = K.sum(pred) + K.epsilon()\n\n    precision = correctPositives / predictedPositives\n    recall = correctPositives / groundPositives\n\n    m = (2 * precision * recall) / (precision + recall)\n\n    return m",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv('../input/quora-insincere-questions-classification/train.csv', usecols=['question_text', 'target'])\nval_df = pd.read_csv('../input/train-test-result-analysis/val_data.csv')",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73cdb32db01c5ec34c709e4cc6576726de8abf3d"
      },
      "cell_type": "code",
      "source": "val_idx = val_df['column_index'].values\nbad_df = train_df.index.isin(val_idx)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9500bc452131666d04cff1b34e17ff67bfce1425"
      },
      "cell_type": "code",
      "source": "val_df = train_df[bad_df]\ntrain_df = train_df[~bad_df]\n\nprint(train_df.shape, val_df.shape)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(1249752, 2) (56370, 2)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5974e0bcc9641289643a1381b362c08c622450d"
      },
      "cell_type": "code",
      "source": "special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n\ndef clean_text(x):\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3be6edd9d062c7bc9821f725e49531e615888efc"
      },
      "cell_type": "code",
      "source": "train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text(str(x)))\nval_df['question_text'] = val_df['question_text'].apply(lambda x: clean_text(str(x)))",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a84b3f8117091395563ea35368c3e8bb32f5ea36"
      },
      "cell_type": "code",
      "source": "train_sentences = train_df['question_text']\ntrain_labels = train_df['target']\nval_sentences = val_df['question_text']\nval_labels = val_df['target']",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4f82c889b47596aed2403e8fe34d25e0b65282c"
      },
      "cell_type": "code",
      "source": "def add_features(df):\n    print('Processing features for dataframe!\\n')\n    df['question_text'] = df['question_text'].apply(lambda x:str(x))\n    for text_function in text_function_list:\n        start_time = time.time()\n        df[text_function.__name__] = df['question_text'].apply(lambda x: text_function(str(x)))\n        print('Processing features for function {} took {} minutes'.format(text_function.__name__, (time.time()-start_time)/60))\n    df['total_length'] = df['question_text'].apply(len)\n    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/(float(row['total_length'])+1),\n                                axis=1)\n    df['num_words'] = df['question_text'].str.count('\\S+')\n    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / (df['num_words']+1)\n    del df['num_unique_words'], df['num_words'], df['capitals'], df['total_length']\n    gc.collect()\n    print('Done!\\n')\n    return df",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72c4bc8eaa997a44b8b7aa12f310b515863309d6"
      },
      "cell_type": "code",
      "source": "start = time.time()\ntrain_df = add_features(train_df)\nprint('\\nTook {} minutes to process train\\n'.format((time.time()-start)/60))\nval_df = add_features(val_df)\nprint('\\nTook {} minutes to process val\\n'.format((time.time()-start)/60))\nprint('\\n\\nTook {} minutes total'.format((time.time()-start)/60))",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processing features for dataframe!\n\nProcessing features for function char_count took 0.029738124211629233 minutes\nProcessing features for function lexicon_count took 0.031287304560343426 minutes\nProcessing features for function syllable_count took 1.323587973912557 minutes\nProcessing features for function sentence_count took 0.12240836620330811 minutes\nProcessing features for function avg_letter_per_word took 0.07474201917648315 minutes\nProcessing features for function avg_sentence_length took 0.1651384393374125 minutes\nProcessing features for function avg_sentence_per_word took 0.16396253903706867 minutes\nProcessing features for function avg_syllables_per_word took 1.3883389075597128 minutes\nProcessing features for function flesch_kincaid_grade took 1.6005149682362874 minutes\nProcessing features for function flesch_reading_ease took 1.5901175697644552 minutes\nProcessing features for function polysyllabcount took 1.4625470360120139 minutes\nProcessing features for function smog_index took 0.19618438482284545 minutes\nProcessing features for function coleman_liau_index took 0.29908864498138427 minutes\nProcessing features for function automated_readability_index took 0.2450554092725118 minutes\nProcessing features for function linsear_write_formula took 1.6426640391349792 minutes\nDone!\n\n\nTook 11.167704260349273 minutes to process train\n\nProcessing features for dataframe!\n\nProcessing features for function char_count took 0.0013416488965352377 minutes\nProcessing features for function lexicon_count took 0.0014085173606872559 minutes\nProcessing features for function syllable_count took 0.06005559762318929 minutes\nProcessing features for function sentence_count took 0.005755984783172607 minutes\nProcessing features for function avg_letter_per_word took 0.0034976800282796225 minutes\nProcessing features for function avg_sentence_length took 0.007638335227966309 minutes\nProcessing features for function avg_sentence_per_word took 0.007601439952850342 minutes\nProcessing features for function avg_syllables_per_word took 0.06337785720825195 minutes\nProcessing features for function flesch_kincaid_grade took 0.07186233599980672 minutes\nProcessing features for function flesch_reading_ease took 0.07151161432266236 minutes\nProcessing features for function polysyllabcount took 0.06526615619659423 minutes\nProcessing features for function smog_index took 0.007248274485270182 minutes\nProcessing features for function coleman_liau_index took 0.013407822450002034 minutes\nProcessing features for function automated_readability_index took 0.011129252115885417 minutes\nProcessing features for function linsear_write_formula took 0.0735270063082377 minutes\nDone!\n\n\nTook 11.670907664299012 minutes to process val\n\n\n\nTook 11.67090961933136 minutes total\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51406ddbbe401d01f68a1224687ce9e3d535a892"
      },
      "cell_type": "code",
      "source": "val_df.head()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "                                        question_text       ...         words_vs_unique\n13  Can we use our external hard disk as a OS as w...       ...                0.809524\n19  How many baronies might exist within a county ...       ...                0.900000\n30  Which babies are more sweeter to their parents...       ...                0.812500\n36  Why my package still is ISC since May , and I ...       ...                0.933333\n76         Can we get ITC on charges levied by banks?       ...                0.900000\n\n[5 rows x 19 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question_text</th>\n      <th>target</th>\n      <th>char_count</th>\n      <th>lexicon_count</th>\n      <th>syllable_count</th>\n      <th>sentence_count</th>\n      <th>avg_letter_per_word</th>\n      <th>avg_sentence_length</th>\n      <th>avg_sentence_per_word</th>\n      <th>avg_syllables_per_word</th>\n      <th>flesch_kincaid_grade</th>\n      <th>flesch_reading_ease</th>\n      <th>polysyllabcount</th>\n      <th>smog_index</th>\n      <th>coleman_liau_index</th>\n      <th>automated_readability_index</th>\n      <th>linsear_write_formula</th>\n      <th>caps_vs_length</th>\n      <th>words_vs_unique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>Can we use our external hard disk as a OS as w...</td>\n      <td>0</td>\n      <td>77</td>\n      <td>20</td>\n      <td>26</td>\n      <td>1</td>\n      <td>3.85</td>\n      <td>20.0</td>\n      <td>0.05</td>\n      <td>1.3</td>\n      <td>7.6</td>\n      <td>76.56</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>5.05</td>\n      <td>6.7</td>\n      <td>13.00</td>\n      <td>0.030928</td>\n      <td>0.809524</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>How many baronies might exist within a county ...</td>\n      <td>0</td>\n      <td>47</td>\n      <td>9</td>\n      <td>17</td>\n      <td>1</td>\n      <td>5.22</td>\n      <td>9.0</td>\n      <td>0.11</td>\n      <td>1.9</td>\n      <td>10.3</td>\n      <td>36.96</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>11.22</td>\n      <td>7.7</td>\n      <td>5.50</td>\n      <td>0.017857</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Which babies are more sweeter to their parents...</td>\n      <td>1</td>\n      <td>72</td>\n      <td>15</td>\n      <td>20</td>\n      <td>2</td>\n      <td>4.80</td>\n      <td>7.5</td>\n      <td>0.13</td>\n      <td>1.3</td>\n      <td>2.7</td>\n      <td>89.24</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>8.19</td>\n      <td>4.9</td>\n      <td>2.75</td>\n      <td>0.022989</td>\n      <td>0.812500</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Why my package still is ISC since May , and I ...</td>\n      <td>0</td>\n      <td>51</td>\n      <td>14</td>\n      <td>16</td>\n      <td>1</td>\n      <td>3.64</td>\n      <td>14.0</td>\n      <td>0.07</td>\n      <td>1.1</td>\n      <td>2.9</td>\n      <td>99.57</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>3.24</td>\n      <td>2.7</td>\n      <td>7.00</td>\n      <td>0.092308</td>\n      <td>0.933333</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>Can we get ITC on charges levied by banks?</td>\n      <td>0</td>\n      <td>34</td>\n      <td>9</td>\n      <td>10</td>\n      <td>1</td>\n      <td>3.78</td>\n      <td>9.0</td>\n      <td>0.11</td>\n      <td>1.1</td>\n      <td>0.9</td>\n      <td>104.64</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>2.87</td>\n      <td>0.9</td>\n      <td>3.50</td>\n      <td>0.093023</td>\n      <td>0.900000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06850a2bf7c50adf4ba121ec6cf468431def71ac"
      },
      "cell_type": "code",
      "source": "train_features = train_df.drop(['question_text', 'target'], axis=1)\nval_features = val_df.drop(['question_text', 'target'], axis=1)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a74a50176462462f839195f32a98c3f6f1987d9c"
      },
      "cell_type": "code",
      "source": "ss = StandardScaler()\nss.fit(np.vstack((train_features, val_features)))\ntrain_features = ss.transform(train_features)\nval_features = ss.transform(val_features)",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3754d4d288fe5411ee0cc26112a17dd34a6e427c",
        "_cell_guid": "03b49435-b06b-4f72-b8f2-27faae8cf478",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "del ss, train_df, val_df\ngc.collect()",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "141"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ab720adb979acbb71bdbc1b0baf975b592049ea"
      },
      "cell_type": "code",
      "source": "max_features = 20000\nmaxlen = 100",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fafbeb9c7604d9306ee0b11831769b616bb3b49"
      },
      "cell_type": "code",
      "source": "tokenizer = text.Tokenizer(num_words=max_features)",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9ebc89a3e784567de70e9b77b514e0553e75a43"
      },
      "cell_type": "code",
      "source": "tokenizer.fit_on_texts(list(train_sentences) + list(val_sentences))",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a228c36b8fad3cf02e19cbb43210bb10735807a2"
      },
      "cell_type": "code",
      "source": "tokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62da3970827adfad76ff96f80559478c11057516"
      },
      "cell_type": "code",
      "source": "tokenized_val = tokenizer.texts_to_sequences(val_sentences)\nX_val = sequence.pad_sequences(tokenized_val, maxlen=maxlen)",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f187774394ff4528962639ac2b5f0e831c6b6475"
      },
      "cell_type": "code",
      "source": "del tokenized_val, tokenized_train, train_sentences, val_sentences\ngc.collect()",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a26f3d41b7aa7efd2144129b8aba7da3621ba24b"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77d7a6375f364f34bd1834059bbc724c36f0a908"
      },
      "cell_type": "code",
      "source": "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3346b036d8411f16e60252321b0d643b3e7331c8"
      },
      "cell_type": "code",
      "source": "all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51eeee235b1842de4ba53f122df5ffad9dbd6d4d"
      },
      "cell_type": "code",
      "source": "del word_index, embeddings_index, all_embs, tokenizer, nb_words\ngc.collect()",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d23cba7120d3b8c206801fa19e8754c6de44dc9"
      },
      "cell_type": "code",
      "source": "batch_size = 1024\nepochs = 2\nembed_size = 300",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c36d39a2094cbaa7e306af22cd870e986f473ff"
      },
      "cell_type": "code",
      "source": "def dnn_model(features, train_flag = True):\n    features_input = layers.Input(shape=(features.shape[1],))\n    inp = layers.Input(shape=(maxlen, ))\n    x = layers.Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_flag)(inp)\n    x = layers.Bidirectional(layers.CuDNNLSTM(64, kernel_initializer='glorot_normal', return_sequences = True))(x)\n    x, x_h, x_c = layers.Bidirectional(layers.CuDNNGRU(64, kernel_initializer='glorot_normal', return_sequences=True, return_state = True))(x)\n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool, x_h, max_pool, features_input])\n    x = layers.Dense(32, activation=\"tanh\", kernel_initializer='glorot_normal')(x)\n    x = layers.Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_normal')(x)\n    model = Model(inputs=[inp,features_input], outputs=x)\n    adam = optimizers.adam(clipvalue=1.0)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=[f1_score])\n\n    return model",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9fbb2b2b63e41f45c15ae103f8ae05b5549747c"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, False)\nmodel.summary()",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            (None, 100)          0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 100, 300)     6000000     input_2[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 100, 128)     187392      embedding_1[0][0]                \n__________________________________________________________________________________________________\nbidirectional_2 (Bidirectional) [(None, 100, 128), ( 74496       bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nglobal_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\nglobal_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            (None, 17)           0                                            \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 337)          0           global_average_pooling1d_1[0][0] \n                                                                 bidirectional_2[0][1]            \n                                                                 global_max_pooling1d_1[0][0]     \n                                                                 input_1[0][0]                    \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 32)           10816       concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n==================================================================================================\nTotal params: 6,272,737\nTrainable params: 272,737\nNon-trainable params: 6,000,000\n__________________________________________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cc9437fc6b557e4fcfca155abb10c00e0935798"
      },
      "cell_type": "code",
      "source": "weight_path=\"early_weights.hdf5\"\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\nearly_stopping = EarlyStopping(monitor=\"val_f1_score\", mode=\"max\", patience=4)\n#clr = CyclicLR(base_lr=0.0003, max_lr=0.005, step_size=2000.)\ncallbacks = [checkpoint, early_stopping]",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e26f4765c718fd8f57773cf8725487cb79a68d40"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 272s 217us/step - loss: 0.1201 - f1_score: 0.5439 - val_loss: 0.1119 - val_f1_score: 0.6253\n\nEpoch 00001: val_f1_score improved from -inf to 0.62529, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 271s 217us/step - loss: 0.1043 - f1_score: 0.6272 - val_loss: 0.1090 - val_f1_score: 0.6191\n\nEpoch 00002: val_f1_score did not improve from 0.62529\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 34,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f51b29e0b38>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "171460b6e381182dc94f4b56e40e208646ff7928"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features)\nmodel.summary()",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            (None, 100)          0                                            \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, 100, 300)     6000000     input_4[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_3 (Bidirectional) (None, 100, 128)     187392      embedding_2[0][0]                \n__________________________________________________________________________________________________\nbidirectional_4 (Bidirectional) [(None, 100, 128), ( 74496       bidirectional_3[0][0]            \n__________________________________________________________________________________________________\nglobal_average_pooling1d_2 (Glo (None, 128)          0           bidirectional_4[0][0]            \n__________________________________________________________________________________________________\nglobal_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_4[0][0]            \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            (None, 17)           0                                            \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 337)          0           global_average_pooling1d_2[0][0] \n                                                                 bidirectional_4[0][1]            \n                                                                 global_max_pooling1d_2[0][0]     \n                                                                 input_3[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 32)           10816       concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 1)            33          dense_3[0][0]                    \n==================================================================================================\nTotal params: 6,272,737\nTrainable params: 6,272,737\nNon-trainable params: 0\n__________________________________________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "911482163a99324763fd36335fbdf1f5bed4ceee"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "569cb3e0b9ed8c1dbbfdac50e4af24bb6c36c87a"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 302s 242us/step - loss: 0.1025 - f1_score: 0.6352 - val_loss: 0.1055 - val_f1_score: 0.6574\n\nEpoch 00001: val_f1_score improved from 0.62529 to 0.65739, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 301s 241us/step - loss: 0.0913 - f1_score: 0.6811 - val_loss: 0.1052 - val_f1_score: 0.6492\n\nEpoch 00002: val_f1_score did not improve from 0.65739\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f51a0254160>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bdd2e92d053384cc9c1b1c5c8a2ee0c877d8f7d"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a86e8ff56e9219c4265bb47f11b08052629a15e9"
      },
      "cell_type": "code",
      "source": "val_preds = model.predict([X_val, val_features], batch_size=1024, verbose=1)\nmax_f1_score = 0\nmax_f1_threshold = ''\nfor thresh in np.arange(0.1, 0.901, 0.01):\n    thresh = np.round(thresh, 2)\n    f1_at_threshold = metrics.f1_score(val_labels, (val_preds>thresh).astype(int))\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, f1_at_threshold))\n    if f1_at_threshold > max_f1_score:\n        max_f1_score = f1_at_threshold\n        max_f1_threshold = thresh",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": "56370/56370 [==============================] - 5s 83us/step\nF1 score at threshold 0.1 is 0.5699185583087854\nF1 score at threshold 0.11 is 0.5806794996895236\nF1 score at threshold 0.12 is 0.5881288454578357\nF1 score at threshold 0.13 is 0.5949285384970032\nF1 score at threshold 0.14 is 0.6025725284010892\nF1 score at threshold 0.15 is 0.610441767068273\nF1 score at threshold 0.16 is 0.6147723963894012\nF1 score at threshold 0.17 is 0.6214109521460286\nF1 score at threshold 0.18 is 0.626638647052937\nF1 score at threshold 0.19 is 0.6320284697508897\nF1 score at threshold 0.2 is 0.6371316711312589\nF1 score at threshold 0.21 is 0.6401835053696173\nF1 score at threshold 0.22 is 0.6421610169491525\nF1 score at threshold 0.23 is 0.6452720832886122\nF1 score at threshold 0.24 is 0.6485782504883874\nF1 score at threshold 0.25 is 0.6520307354555435\nF1 score at threshold 0.26 is 0.6543113971812229\nF1 score at threshold 0.27 is 0.6560645089035726\nF1 score at threshold 0.28 is 0.6600090785292783\nF1 score at threshold 0.29 is 0.6630696820112502\nF1 score at threshold 0.3 is 0.663807094829585\nF1 score at threshold 0.31 is 0.6672138836772985\nF1 score at threshold 0.32 is 0.6683256309989334\nF1 score at threshold 0.33 is 0.6699354221478115\nF1 score at threshold 0.34 is 0.6729332369245602\nF1 score at threshold 0.35 is 0.6740425531914894\nF1 score at threshold 0.36 is 0.6736298844925043\nF1 score at threshold 0.37 is 0.6727002231589388\nF1 score at threshold 0.38 is 0.6740110165247872\nF1 score at threshold 0.39 is 0.6738305941845765\nF1 score at threshold 0.4 is 0.6745788667687596\nF1 score at threshold 0.41 is 0.6735377480030921\nF1 score at threshold 0.42 is 0.6717895285230528\nF1 score at threshold 0.43 is 0.6725292033075206\nF1 score at threshold 0.44 is 0.6730158730158731\nF1 score at threshold 0.45 is 0.6699546787523327\nF1 score at threshold 0.46 is 0.6699811878527279\nF1 score at threshold 0.47 is 0.6687483030138475\nF1 score at threshold 0.48 is 0.6669410150891633\nF1 score at threshold 0.49 is 0.6631637168141593\nF1 score at threshold 0.5 is 0.6608986882500697\nF1 score at threshold 0.51 is 0.6594396733774461\nF1 score at threshold 0.52 is 0.6583179166073715\nF1 score at threshold 0.53 is 0.6555491661874641\nF1 score at threshold 0.54 is 0.65039941902687\nF1 score at threshold 0.55 is 0.6460682383950798\nF1 score at threshold 0.56 is 0.6425500370644922\nF1 score at threshold 0.57 is 0.6400719856028794\nF1 score at threshold 0.58 is 0.6353155339805825\nF1 score at threshold 0.59 is 0.6298224127372933\nF1 score at threshold 0.6 is 0.6253879577901924\nF1 score at threshold 0.61 is 0.6206571293821727\nF1 score at threshold 0.62 is 0.6130109750278351\nF1 score at threshold 0.63 is 0.6057971014492753\nF1 score at threshold 0.64 is 0.599282686664493\nF1 score at threshold 0.65 is 0.5929101401483925\nF1 score at threshold 0.66 is 0.5862529195862529\nF1 score at threshold 0.67 is 0.5820266396897656\nF1 score at threshold 0.68 is 0.5737228771570135\nF1 score at threshold 0.69 is 0.5640402637972927\nF1 score at threshold 0.7 is 0.5588338602037233\nF1 score at threshold 0.71 is 0.5460385438972163\nF1 score at threshold 0.72 is 0.5351194786386676\nF1 score at threshold 0.73 is 0.5216593245227605\nF1 score at threshold 0.74 is 0.5101395348837209\nF1 score at threshold 0.75 is 0.49820449820449825\nF1 score at threshold 0.76 is 0.48193697156033816\nF1 score at threshold 0.77 is 0.464034401876466\nF1 score at threshold 0.78 is 0.4432044639298526\nF1 score at threshold 0.79 is 0.42239805234327454\nF1 score at threshold 0.8 is 0.40214920438107044\nF1 score at threshold 0.81 is 0.3789251844046365\nF1 score at threshold 0.82 is 0.35608180839612485\nF1 score at threshold 0.83 is 0.334209373631187\nF1 score at threshold 0.84 is 0.30513798519183305\nF1 score at threshold 0.85 is 0.27433831990794016\nF1 score at threshold 0.86 is 0.23837620958225159\nF1 score at threshold 0.87 is 0.20777965692196182\nF1 score at threshold 0.88 is 0.17255871446229912\nF1 score at threshold 0.89 is 0.139099645928174\nF1 score at threshold 0.9 is 0.10542635658914729\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a439262df724edf2da5151d2dee7300c66470a22"
      },
      "cell_type": "markdown",
      "source": "### With val_acc as metric"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3433fc94d2ff945d5884e0d05a688938691656d2"
      },
      "cell_type": "code",
      "source": "print('Max threshold is {} with f1 score of {}'.format(max_f1_threshold, max_f1_score))",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Max threshold is 0.4 with f1 score of 0.6745788667687596\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "133fc923e76c2bc2a7d221cd23e1da9bd0a7114b"
      },
      "cell_type": "markdown",
      "source": "### With f1 metric as val_check"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63d6d83c00beec69b63208543cbcf35f7e27fc52"
      },
      "cell_type": "code",
      "source": "print('Max threshold is {} with f1 score of {}'.format(max_f1_threshold, max_f1_score))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6256a8b29fb84479e202651aec40cdc58083229a"
      },
      "cell_type": "markdown",
      "source": "### With CLR"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb4825b85179faf0dfda765ad03ff0caf34f3f38"
      },
      "cell_type": "code",
      "source": "print('Max threshold is {} with f1 score of {}'.format(max_f1_threshold, max_f1_score))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "196182871f52788561946fa251d218c3a93997a0"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}