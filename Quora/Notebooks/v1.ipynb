{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import gc\nimport re\nimport os\nimport pandas as pd\nimport numpy as np\nfrom unidecode import unidecode\nfrom sklearn.preprocessing import StandardScaler\nprint(os.listdir(\"../input\"))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a2b61717e468b5a6b9820240af45236260c7168"
      },
      "cell_type": "code",
      "source": "from keras.models import Model, Sequential\nfrom keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPooling1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM, CuDNNGRU, Bidirectional, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras import activations, initializers, regularizers, constraints\nfrom keras.constraints import maxnorm\nfrom keras.callbacks import Callback\nfrom keras import optimizers",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "639b9ea1e17020b519446c944f3a59b501c6d64a"
      },
      "cell_type": "code",
      "source": "def f1_score(true,pred): #considering sigmoid activation, threshold = 0.5\n    pred = K.cast(K.greater(pred,0.5), K.floatx())\n\n    groundPositives = K.sum(true) + K.epsilon()\n    correctPositives = K.sum(true * pred) + K.epsilon()\n    predictedPositives = K.sum(pred) + K.epsilon()\n\n    precision = correctPositives / predictedPositives\n    recall = correctPositives / groundPositives\n\n    m = (2 * precision * recall) / (precision + recall)\n\n    return m",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv('../input/train.csv', usecols=['question_text', 'target'])\ntest_df = pd.read_csv('../input/test.csv', usecols = ['question_text'])",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5974e0bcc9641289643a1381b362c08c622450d"
      },
      "cell_type": "code",
      "source": "special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n\ndef clean_text(x):\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3be6edd9d062c7bc9821f725e49531e615888efc"
      },
      "cell_type": "code",
      "source": "train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text(str(x)))\ntest_df['question_text'] = test_df['question_text'].apply(lambda x: clean_text(str(x)))\n\ntrain_sentences = train_df['question_text']\ntrain_labels = train_df['target']\ntest_sentences = test_df['question_text']",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4f82c889b47596aed2403e8fe34d25e0b65282c"
      },
      "cell_type": "code",
      "source": "def add_features(df):\n    \n    df['question_text'] = df['question_text'].apply(lambda x:str(x))\n    df['total_length'] = df['question_text'].apply(len)\n    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/(float(row['total_length'])+1),\n                                axis=1)\n    df['num_words'] = df['question_text'].str.count('\\S+')\n    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / (df['num_words']+1)\n    del df['num_unique_words'], df['num_words'], df['capitals'], df['total_length']\n    return df",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72c4bc8eaa997a44b8b7aa12f310b515863309d6"
      },
      "cell_type": "code",
      "source": "train_df = add_features(train_df)\ntest_df = add_features(test_df)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06850a2bf7c50adf4ba121ec6cf468431def71ac"
      },
      "cell_type": "code",
      "source": "train_features = train_df[['caps_vs_length', 'words_vs_unique']].fillna(0)\ntest_features = test_df[['caps_vs_length', 'words_vs_unique']].fillna(0)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a74a50176462462f839195f32a98c3f6f1987d9c"
      },
      "cell_type": "code",
      "source": "ss = StandardScaler()\nss.fit(np.vstack((train_features, test_features)))\ntrain_features = ss.transform(train_features)\ntest_features = ss.transform(test_features)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3754d4d288fe5411ee0cc26112a17dd34a6e427c",
        "_cell_guid": "03b49435-b06b-4f72-b8f2-27faae8cf478",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "gc.collect()",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "155"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ab720adb979acbb71bdbc1b0baf975b592049ea"
      },
      "cell_type": "code",
      "source": "max_features = 20000\nmaxlen = 100",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fafbeb9c7604d9306ee0b11831769b616bb3b49"
      },
      "cell_type": "code",
      "source": "tokenizer = text.Tokenizer(num_words=max_features)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9ebc89a3e784567de70e9b77b514e0553e75a43"
      },
      "cell_type": "code",
      "source": "tokenizer.fit_on_texts(list(train_sentences) + list(test_sentences))",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a228c36b8fad3cf02e19cbb43210bb10735807a2"
      },
      "cell_type": "code",
      "source": "tokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29ff082b690cc6ef49136732bc12e6adf95ef693"
      },
      "cell_type": "code",
      "source": "tokenized_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a26f3d41b7aa7efd2144129b8aba7da3621ba24b"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77d7a6375f364f34bd1834059bbc724c36f0a908"
      },
      "cell_type": "code",
      "source": "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3346b036d8411f16e60252321b0d643b3e7331c8"
      },
      "cell_type": "code",
      "source": "all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51eeee235b1842de4ba53f122df5ffad9dbd6d4d"
      },
      "cell_type": "code",
      "source": "del word_index, embeddings_index, all_embs, tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences, nb_words\ngc.collect()",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d23cba7120d3b8c206801fa19e8754c6de44dc9"
      },
      "cell_type": "code",
      "source": "batch_size = 1024\nepochs = 8\nembed_size = 300",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1c413e9230e1b81c2082b651cd2e90fa9a6bfaf"
      },
      "cell_type": "code",
      "source": "gc.collect()",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c36d39a2094cbaa7e306af22cd870e986f473ff"
      },
      "cell_type": "code",
      "source": "def cudnnlstm_model(features):\n    features_input = Input(shape=(features.shape[1],))\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n    x = Bidirectional(CuDNNLSTM(64, kernel_initializer='glorot_normal', return_sequences = True))(x)\n    x, x_h, x_c = Bidirectional(CuDNNGRU(64, kernel_initializer='glorot_normal', return_sequences=True, return_state = True))(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    x = concatenate([avg_pool, x_h, max_pool, features_input])\n    x = Dense(32, activation=\"tanh\", kernel_initializer='glorot_normal')(x)\n    x = Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_normal')(x)\n    model = Model(inputs=[inp,features_input], outputs=x)\n    adam = optimizers.adam(clipvalue=1.0)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=[f1_score])\n\n    return model",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9fbb2b2b63e41f45c15ae103f8ae05b5549747c"
      },
      "cell_type": "code",
      "source": "cudnnlstm_model = cudnnlstm_model(train_features)\ncudnnlstm_model.summary()",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            (None, 100)          0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 100, 300)     6000000     input_2[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 100, 128)     187392      embedding_1[0][0]                \n__________________________________________________________________________________________________\nbidirectional_2 (Bidirectional) [(None, 100, 128), ( 74496       bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nglobal_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\nglobal_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            (None, 2)            0                                            \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 322)          0           global_average_pooling1d_1[0][0] \n                                                                 bidirectional_2[0][1]            \n                                                                 global_max_pooling1d_1[0][0]     \n                                                                 input_1[0][0]                    \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 32)           10336       concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n==================================================================================================\nTotal params: 6,272,257\nTrainable params: 6,272,257\nNon-trainable params: 0\n__________________________________________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cc9437fc6b557e4fcfca155abb10c00e0935798"
      },
      "cell_type": "code",
      "source": "weight_path=\"early_weights.hdf5\"\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\nearly_stopping = EarlyStopping(monitor=\"val_f1_score\", mode=\"max\", patience=3)\ncallbacks = [checkpoint, early_stopping]",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e26f4765c718fd8f57773cf8725487cb79a68d40"
      },
      "cell_type": "code",
      "source": "cudnnlstm_model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=2, shuffle = True, validation_split=0.20, callbacks=callbacks)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6197ef311e5644424397a66fb5dd287f9040d4c3"
      },
      "cell_type": "code",
      "source": "cudnnlstm_model.load_weights(weight_path)\ny_pred = cudnnlstm_model.predict([X_test,test_features], batch_size=batch_size)\ny_pred = [x for i in y_pred for x in i]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61d38989f67f28214873139c0c315d9bfe08df79"
      },
      "cell_type": "code",
      "source": "sample = pd.read_csv('../input/sample_submission.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c5f8a9d03a2910198da606065da1c95d3ac40ef"
      },
      "cell_type": "code",
      "source": "sample['prediction'] = pd.Series(y_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d8d79b6ff4c9507795cade0294d15672e9b9e75"
      },
      "cell_type": "code",
      "source": "sample['prediction'] = sample['prediction'].apply(lambda x: 0 if x <= 0.5 else 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c869113905555fb3de30c46d8cb3b6211bc61264"
      },
      "cell_type": "code",
      "source": "sample.to_csv('submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55aa3f3f8a0b9ad67e3ffae03f3dec5e9129310d"
      },
      "cell_type": "code",
      "source": "sample['prediction'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a86e8ff56e9219c4265bb47f11b08052629a15e9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}