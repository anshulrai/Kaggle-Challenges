{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import gc\nimport re\nimport os\nimport pandas as pd\nimport numpy as np\nfrom unidecode import unidecode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nimport string\nimport math\nimport operator\nfrom pyphen import Pyphen\nimport time\nprint(os.listdir(\"../input\"))",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a2b61717e468b5a6b9820240af45236260c7168"
      },
      "cell_type": "code",
      "source": "from keras.models import Model, Sequential\nfrom keras import layers\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras import backend as K\nfrom keras import optimizers",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3334066cad56e6bc24f7060499bb332183338a44"
      },
      "cell_type": "code",
      "source": "class CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "639b9ea1e17020b519446c944f3a59b501c6d64a"
      },
      "cell_type": "code",
      "source": "def f1_score(true,pred): #considering sigmoid activation, threshold = 0.5\n    pred = K.cast(K.greater(pred,0.5), K.floatx())\n\n    groundPositives = K.sum(true) + K.epsilon()\n    correctPositives = K.sum(true * pred) + K.epsilon()\n    predictedPositives = K.sum(pred) + K.epsilon()\n\n    precision = correctPositives / predictedPositives\n    recall = correctPositives / groundPositives\n\n    m = (2 * precision * recall) / (precision + recall)\n\n    return m",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "454b11ebab0f29650f4c8bfe55ebe7bfe1ddb506"
      },
      "cell_type": "code",
      "source": "exclude = list(string.punctuation)\ndic = Pyphen(lang='en_US')\n\ndef legacy_round(number, points=0):\n    p = 10 ** points\n    return float(math.floor((number * p) + math.copysign(0.5, number))) / p\n\ndef char_count(text, ignore_spaces=True):\n        if ignore_spaces:\n            text = text.replace(\" \", \"\")\n        return len(text)\n\ndef lexicon_count(text):\n        count = len(text.split())\n        return count\n    \ndef syllable_count(text):\n        text = text.lower()\n        text = \"\".join(x for x in text if x not in exclude)\n        if not text:\n            return 0\n        count = 0\n        vowels = 'aeiouy'\n        for word in text.split(' '):\n            #word_hyphenated = dic.inserted(word)\n            #count += max(1, word_hyphenated.count(\"-\") + 1)\n            word = word.strip(\".:;?!\")\n            if len(word) < 1:\n                continue\n            if word[0] in vowels:\n                count +=1\n            for index in range(1,len(word)):\n                if word[index] in vowels and word[index-1] not in vowels:\n                    count +=1\n            if word.endswith('e'):\n                count -= 1\n            if word.endswith('le'):\n                count+=1\n            if count == 0:\n                count +=1\n        return count\n\ndef sentence_count(text):\n        ignore_count = 0\n        sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]*[ |\\n](?=[A-Z])', text)\n        for sentence in sentences:\n            if lexicon_count(sentence) <= 2:\n                ignore_count += 1\n        return max(1, len(sentences) - ignore_count)\n    \ndef avg_sentence_length(text):\n        try:\n            asl = float(lexicon_count(text) / sentence_count(text))\n            return legacy_round(asl, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_syllables_per_word(text):\n        syllable = syllable_count(text)\n        words = lexicon_count(text)\n        try:\n            syllables_per_word = float(syllable) / float(words)\n            return legacy_round(syllables_per_word, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_letter_per_word(text):\n        try:\n            letters_per_word = float(\n                char_count(text) / lexicon_count(text))\n            return legacy_round(letters_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n\n\ndef avg_sentence_per_word(text):\n        try:\n            sentence_per_word = float(\n                sentence_count(text) / lexicon_count(text))\n            return legacy_round(sentence_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n        \ndef flesch_reading_ease(text):\n        sentence_length = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = (\n            206.835\n            - float(1.015 * sentence_length)\n            - float(84.6 * syllables_per_word)\n        )\n        return legacy_round(flesch, 2)\n\ndef flesch_kincaid_grade(text):\n        sentence_lenth = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = (\n            float(0.39 * sentence_lenth)\n            + float(11.8 * syllables_per_word)\n            - 15.59)\n        return legacy_round(flesch, 1)\n\ndef polysyllabcount(text):\n        count = 0\n        for word in text.split():\n            wrds = syllable_count(word)\n            if wrds >= 3:\n                count += 1\n        return count\n\ndef smog_index(text):\n        sentences = sentence_count(text)\n        if sentences >= 3:\n            try:\n                poly_syllab = polysyllabcount(text)\n                smog = (\n                    (1.043 * (30 * (poly_syllab / sentences)) ** .5)\n                    + 3.1291)\n                return legacy_round(smog, 1)\n            except ZeroDivisionError:\n                return 0.0\n        else:\n            return 0.0\n\ndef coleman_liau_index(text):\n        letters = legacy_round(avg_letter_per_word(text)*100, 2)\n        sentences = legacy_round(avg_sentence_per_word(text)*100, 2)\n        coleman = float((0.058 * letters) - (0.296 * sentences) - 15.8)\n        return legacy_round(coleman, 2)\n\ndef automated_readability_index(text):\n        chrs = char_count(text)\n        words = lexicon_count(text)\n        sentences = sentence_count(text)\n        try:\n            a = float(chrs)/float(words)\n            b = float(words) / float(sentences)\n            readability = (\n                (4.71 * legacy_round(a, 2))\n                + (0.5 * legacy_round(b, 2))\n                - 21.43)\n            return legacy_round(readability, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef linsear_write_formula(text):\n        easy_word = 0\n        difficult_word = 0\n        text_list = text.split()[:100]\n        for word in text_list:\n            if syllable_count(word) < 3:\n                easy_word += 1\n            else:\n                difficult_word += 1\n        text = ' '.join(text_list)\n        number = float(\n            (easy_word * 1 + difficult_word * 3)\n            / sentence_count(text))\n        if number <= 20:\n            number -= 2\n        return number / 2",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77c7638fe671a1872e57afba27cc9d7ad7e75f51"
      },
      "cell_type": "code",
      "source": "text_function_list = [char_count, lexicon_count, syllable_count, sentence_count, avg_letter_per_word,\n                     avg_sentence_length, avg_sentence_per_word, avg_syllables_per_word, flesch_kincaid_grade, flesch_reading_ease,\n                     polysyllabcount, smog_index, coleman_liau_index, automated_readability_index, linsear_write_formula]",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv('../input/train.csv', usecols=['question_text', 'target'])\ntest_df = pd.read_csv('../input/test.csv', usecols = ['question_text'])",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5974e0bcc9641289643a1381b362c08c622450d"
      },
      "cell_type": "code",
      "source": "special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n\ndef clean_text(x):\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3be6edd9d062c7bc9821f725e49531e615888efc"
      },
      "cell_type": "code",
      "source": "train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text(str(x)))\ntest_df['question_text'] = test_df['question_text'].apply(lambda x: clean_text(str(x)))\n\ntrain_sentences = train_df['question_text']\ntrain_labels = train_df['target']\ntest_sentences = test_df['question_text']",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4f82c889b47596aed2403e8fe34d25e0b65282c"
      },
      "cell_type": "code",
      "source": "def add_features(df):\n    df['question_text'] = df['question_text'].apply(lambda x:str(x))\n    for text_function in text_function_list:\n        df[text_function.__name__] = df['question_text'].apply(lambda x: text_function(str(x)))\n    df['total_length'] = df['question_text'].apply(len)\n    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/(float(row['total_length'])+1),\n                                axis=1)\n    df['num_words'] = df['question_text'].str.count('\\S+')\n    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / (df['num_words']+1)\n    del df['num_unique_words'], df['num_words'], df['capitals'], df['total_length']\n    gc.collect()\n    return df",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72c4bc8eaa997a44b8b7aa12f310b515863309d6"
      },
      "cell_type": "code",
      "source": "train_df = add_features(train_df)\ntest_df = add_features(test_df)",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-39f121be0a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-17831cc2563c>\u001b[0m in \u001b[0;36madd_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_function_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'capitals'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-17831cc2563c>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_function_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'capitals'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d55fcb56cfef>\u001b[0m in \u001b[0;36mavg_syllables_per_word\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexicon_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0msyllables_per_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyllable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyllables_per_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06850a2bf7c50adf4ba121ec6cf468431def71ac"
      },
      "cell_type": "code",
      "source": "train_features = train_df.drop(['question_text', 'target'], axis=1)\ntest_features = test_df.drop(['question_text'], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a74a50176462462f839195f32a98c3f6f1987d9c"
      },
      "cell_type": "code",
      "source": "ss = StandardScaler()\nss.fit(np.vstack((train_features, test_features)))\ntrain_features = ss.transform(train_features)\ntest_features = ss.transform(test_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3754d4d288fe5411ee0cc26112a17dd34a6e427c",
        "_cell_guid": "03b49435-b06b-4f72-b8f2-27faae8cf478",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ab720adb979acbb71bdbc1b0baf975b592049ea"
      },
      "cell_type": "code",
      "source": "max_features = 20000\nmaxlen = 100",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fafbeb9c7604d9306ee0b11831769b616bb3b49"
      },
      "cell_type": "code",
      "source": "tokenizer = text.Tokenizer(num_words=max_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9ebc89a3e784567de70e9b77b514e0553e75a43"
      },
      "cell_type": "code",
      "source": "tokenizer.fit_on_texts(list(train_sentences) + list(test_sentences))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a228c36b8fad3cf02e19cbb43210bb10735807a2"
      },
      "cell_type": "code",
      "source": "tokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29ff082b690cc6ef49136732bc12e6adf95ef693"
      },
      "cell_type": "code",
      "source": "tokenized_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40aec451e66054a7ce4fefd0841e1b19c8aa0720"
      },
      "cell_type": "code",
      "source": "gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a26f3d41b7aa7efd2144129b8aba7da3621ba24b"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77d7a6375f364f34bd1834059bbc724c36f0a908"
      },
      "cell_type": "code",
      "source": "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3346b036d8411f16e60252321b0d643b3e7331c8"
      },
      "cell_type": "code",
      "source": "all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51eeee235b1842de4ba53f122df5ffad9dbd6d4d"
      },
      "cell_type": "code",
      "source": "del word_index, embeddings_index, all_embs, tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences, nb_words\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d23cba7120d3b8c206801fa19e8754c6de44dc9"
      },
      "cell_type": "code",
      "source": "batch_size = 1024\nepochs = 3\nembed_size = 300",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c36d39a2094cbaa7e306af22cd870e986f473ff"
      },
      "cell_type": "code",
      "source": "def dnn_model(features, train_flag = True):\n    features_input = layers.Input(shape=(features.shape[1],))\n    inp = layers.Input(shape=(maxlen, ))\n    x = layers.Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_flag)(inp)\n    x = layers.Bidirectional(layers.CuDNNLSTM(64, kernel_initializer='glorot_normal', return_sequences = True))(x)\n    x, x_h, x_c = layers.Bidirectional(layers.CuDNNGRU(64, kernel_initializer='glorot_normal', return_sequences=True, return_state = True))(x)\n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool, x_h, max_pool, features_input])\n    x = layers.Dense(32, activation=\"tanh\", kernel_initializer='glorot_normal')(x)\n    x = layers.Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_normal')(x)\n    model = Model(inputs=[inp,features_input], outputs=x)\n    adam = optimizers.adam(clipvalue=1.0)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=[f1_score])\n\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9fbb2b2b63e41f45c15ae103f8ae05b5549747c"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, False)\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e26f4765c718fd8f57773cf8725487cb79a68d40"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b38dc3c924a4bcbd80f2e306864f9109ba821b4a"
      },
      "cell_type": "code",
      "source": "model.layers[1].trainable = True\nadam = optimizers.adam(clipvalue=1.0)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=[f1_score])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "569cb3e0b9ed8c1dbbfdac50e4af24bb6c36c87a"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6197ef311e5644424397a66fb5dd287f9040d4c3"
      },
      "cell_type": "code",
      "source": "y_pred = model.predict([X_test,test_features], batch_size=batch_size)\ny_pred = [x for i in y_pred for x in i]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61d38989f67f28214873139c0c315d9bfe08df79"
      },
      "cell_type": "code",
      "source": "sample = pd.read_csv('../input/sample_submission.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c5f8a9d03a2910198da606065da1c95d3ac40ef"
      },
      "cell_type": "code",
      "source": "sample['prediction'] = pd.Series(y_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d8d79b6ff4c9507795cade0294d15672e9b9e75"
      },
      "cell_type": "code",
      "source": "sample['prediction'] = sample['prediction'].apply(lambda x: 0 if x <= 0.35 else 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c869113905555fb3de30c46d8cb3b6211bc61264"
      },
      "cell_type": "code",
      "source": "sample.to_csv('submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55aa3f3f8a0b9ad67e3ffae03f3dec5e9129310d"
      },
      "cell_type": "code",
      "source": "sample['prediction'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a86e8ff56e9219c4265bb47f11b08052629a15e9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}