{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport re\nimport pandas as pd\nimport random\nimport numpy as np\nfrom unidecode import unidecode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nimport string\nimport re\nimport math\nimport operator\nimport time\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5a187360988431a99abd0f653146743d0845b3f"},"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = '0'\n    np.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# GENERAL HYPERPARAMS\nnum_folds = 5\nseed = 42\n\n# HYPERPARAMS FOR TEXT PROCESSING\nmax_features = 200000\nmaxlen = 100\n\n# HYPERPARAMS FOR NN\nbatch_size = 1024\nepochs = 2\nembed_size = 300\n\nset_seed(seed)\n\n# PATH TO DATA DIRECTORY\nPATH = \"../input/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e44db4a743c3893ee324de2037dbb025e5312d02"},"cell_type":"code","source":"puncts = {',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√'}\n\ndef clean_text(x):\n    x = str(x)\n    table = str.maketrans({key: ' {punct} ' for key in puncts})\n    return x.translate(table)\n\ndef clean_numbers(x):\n    if bool(re.search(r'\\d', x)):\n        x = re.sub('[0-9]{5,}', '#####', x)\n        x = re.sub('[0-9]{4}', '####', x)\n        x = re.sub('[0-9]{3}', '###', x)\n        x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"125d55f48d7178a3b2893dc35bb900440c0e77af"},"cell_type":"markdown","source":"# Word Features"},{"metadata":{"trusted":true,"_uuid":"2ad39e1a28bdea0475ccc9cd3526295977565da2"},"cell_type":"code","source":"def legacy_round(number, points=0):\n    p = 10 ** points\n    return math.floor((number * p) + math.copysign(0.5, number))/p\n\ndef char_count(text, ignore_spaces=True):\n        if ignore_spaces:\n            text = text.replace(\" \", \"\")\n        return len(text)\n\ndef lexicon_count(text):\n        count = len(text.split())\n        return count\n    \ndef syllable_count(text):\n        text = text.lower()\n        text = \"\".join(x for x in text if x not in list(string.punctuation))\n        if not text:\n            return 0\n        count = 0\n        vowels = {'a', 'e', 'i', 'o', 'u', 'y'}\n        for word in text.split(' '):\n            word = word.strip(\".:;?!\")\n            if len(word) < 1:\n                continue\n            if word[0] in vowels:\n                count +=1\n            for index in range(1,len(word)):\n                if word[index] in vowels and word[index-1] not in vowels:\n                    count +=1\n            if word.endswith('e'):\n                count -= 1\n            if word.endswith('le'):\n                count+=1\n            if count == 0:\n                count +=1\n        return count\n\ndef sentence_count(text):\n        ignore_count = 0\n        sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]*[ |\\n](?=[A-Z])', text)\n        for sentence in sentences:\n            if lexicon_count(sentence) <= 2:\n                ignore_count += 1\n        return max(1, len(sentences) - ignore_count)\n    \ndef avg_sentence_length(text):\n        try:\n            asl = lexicon_count(text) / sentence_count(text)\n            return legacy_round(asl, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_syllables_per_word(text):\n        syllable = syllable_count(text)\n        words = lexicon_count(text)\n        try:\n            syllables_per_word = syllable/words\n            return legacy_round(syllables_per_word, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_letter_per_word(text):\n        try:\n            letters_per_word = char_count(text) / lexicon_count(text)\n            return legacy_round(letters_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_sentence_per_word(text):\n        try:\n            sentence_per_word = sentence_count(text) / lexicon_count(text)\n            return legacy_round(sentence_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n        \ndef flesch_reading_ease(text):\n        sentence_length = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = 206.835 - 1.015 * sentence_length - 84.6 * syllables_per_word\n        return legacy_round(flesch, 2)\n\ndef flesch_kincaid_grade(text):\n        sentence_lenth = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = 0.39 * sentence_lenth + 11.8 * syllables_per_word - 15.59\n        return legacy_round(flesch, 1)\n\ndef polysyllabcount(text):\n        count = 0\n        for word in text.split():\n            wrds = syllable_count(word)\n            if wrds >= 3:\n                count += 1\n        return count\n\ndef smog_index(text):\n        sentences = sentence_count(text)\n        if sentences >= 3:\n            try:\n                poly_syllab = polysyllabcount(text)\n                smog = (1.043 * (30 * (poly_syllab / sentences)) ** .5) + 3.1291\n                return legacy_round(smog, 1)\n            except ZeroDivisionError:\n                return 0.0\n        else:\n            return 0.0\n\ndef coleman_liau_index(text):\n        letters = legacy_round(avg_letter_per_word(text)*100, 2)\n        sentences = legacy_round(avg_sentence_per_word(text)*100, 2)\n        coleman = (0.058 * letters) - (0.296 * sentences) - 15.8\n        return legacy_round(coleman, 2)\n\ndef automated_readability_index(text):\n        chrs = char_count(text)\n        words = lexicon_count(text)\n        sentences = sentence_count(text)\n        try:\n            a = chrs/words\n            b = words /sentences\n            readability = (4.71 * legacy_round(a, 2)) + (0.5 * legacy_round(b, 2)) - 21.43\n            return legacy_round(readability, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef linsear_write_formula(text):\n        easy_word = 0\n        difficult_word = 0\n        text_list = text.split()[:100]\n        for word in text_list:\n            if syllable_count(word) < 3:\n                easy_word += 1\n            else:\n                difficult_word += 1\n        text = ' '.join(text_list)\n        number = (easy_word * 1 + difficult_word * 3)/ sentence_count(text)\n        if number <= 20:\n            number -= 2\n        return number / 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24a29dc3c3569211c61af4f0b3544e44f4e88497"},"cell_type":"markdown","source":"# Metric"},{"metadata":{"trusted":true,"_uuid":"234e79ec68813fb257bf0327dd53d2f1514e4397"},"cell_type":"code","source":"def f1_score(true,pred):\n    #considering sigmoid activation, threshold = 0.5\n    pred = K.cast(K.greater(pred,0.5), K.floatx())\n\n    groundPositives = K.sum(true) + K.epsilon()\n    correctPositives = K.sum(true * pred) + K.epsilon()\n    predictedPositives = K.sum(pred) + K.epsilon()\n\n    precision = correctPositives / predictedPositives\n    recall = correctPositives / groundPositives\n\n    m = (2 * precision * recall) / (precision + recall)\n\n    return m","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbad601c591d2a37408bc81f2c590c861d7f80af"},"cell_type":"markdown","source":"# Misc Functions"},{"metadata":{"trusted":true,"_uuid":"abc6778bc7d3d08de70aa7e32f11877f5afe21fa"},"cell_type":"code","source":"def threshold_search(y_true, y_proba):\n    precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_proba)\n    thresholds = np.append(thresholds, 1.001) \n    F = 2/(1/precision + 1/recall)\n    best_score = np.max(F)\n    best_th = thresholds[np.argmax(F)]\n    search_result = {'threshold': best_th, 'f1': best_score}\n    return search_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c08d1e6069ba82e39ebc01bbe1058d1d71886201"},"cell_type":"code","source":"def clean_text_for_features(x):\n    special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0eb973446bf71f1448d5939caffcbeec4b853c5d"},"cell_type":"code","source":"def add_features(df, function_list):\n    df['question_text'] = df['question_text'].apply(lambda x:str(x))\n    for text_function in function_list:\n        df[text_function.__name__] = df['question_text'].apply(lambda x: text_function(str(x)))\n    df['total_length'] = df['question_text'].apply(len)\n    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/(float(row['total_length'])+1),\n                                axis=1)\n    df['num_words'] = df['question_text'].str.count('\\S+')\n    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / (df['num_words']+1)\n    gc.collect()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7a0386970ec33342c16bd792e5ec793e54f4d7e"},"cell_type":"code","source":"text_function_list = [char_count, lexicon_count, syllable_count, sentence_count, avg_letter_per_word, avg_sentence_length, avg_sentence_per_word, avg_syllables_per_word, flesch_kincaid_grade, flesch_reading_ease, polysyllabcount, smog_index, coleman_liau_index, automated_readability_index, linsear_write_formula]\n#text_function_list = []","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a7b8d9d8012022864542b75514a7965696d86e5"},"cell_type":"markdown","source":"# Begin Main"},{"metadata":{"trusted":true,"_uuid":"8ba6ff3fd7eab7cb4e26c58b3a7a2c000f05601e"},"cell_type":"code","source":"train_df = pd.read_csv(PATH+'train.csv', usecols=['question_text', 'target'])\ntest_df = pd.read_csv(PATH+'test.csv', usecols = ['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96af576b34ccb3962b41fd63981a7689786325fe"},"cell_type":"code","source":"train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text_for_features(str(x)))\ntest_df['question_text'] = test_df['question_text'].apply(lambda x: clean_text_for_features(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ad627cc740a251260f91c55626319167f4efc2e"},"cell_type":"code","source":"%%time\n# CREATE TEXT FEATURES\ntrain_df = add_features(train_df, text_function_list)\ntest_df = add_features(test_df, text_function_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba1ba515b4618853b4be9f62aca727de20f1b04e"},"cell_type":"code","source":"# SAVE AND PROCESS FEATURES TO SEND TO NN\ntrain_features = train_df.drop(['question_text', 'target'], axis=1)\ntest_features = test_df.drop(['question_text'], axis=1)\ntrain_labels = train_df['target']\ndel train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1092cee1938c461e46afa5c1355ee57c509c0fc"},"cell_type":"code","source":"ss = StandardScaler()\nss.fit(np.vstack((train_features, test_features)))\ntrain_features = ss.transform(train_features)\ntest_features = ss.transform(test_features)\n\ndel ss\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76d6daa98936febe3acf76c4fd5863d39f7813da"},"cell_type":"code","source":"train_features.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9f641789b7c76a5b1d634ef7038d97d21f58fb9"},"cell_type":"code","source":"# TO SAVE FINAL PREDICTIONS\nfinal_preds_rf = np.zeros((test_features.shape[0], 1))\nfinal_preds_lgbm = np.zeros((test_features.shape[0], 1))\noof_preds_rf = np.zeros((train_features.shape[0], 1))\noof_preds_lgbm = np.zeros((train_features.shape[0], 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"c35b699fc754d7b23b4886b2f06beac16fd67eb9"},"cell_type":"code","source":"%%time\n# FOLDS FOR CV\nfolds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_features, train_labels)):\n            print('FOLD NUMBER {}:'.format(n_fold+1))\n            train_x, train_y = train_features[train_idx], train_labels[train_idx]\n            valid_x, valid_y = train_features[valid_idx], train_labels[valid_idx]\n            \n            # RF\n            rf = RandomForestClassifier(n_estimators=1400, max_depth=8, random_state=seed, n_jobs=-1, verbose=1)\n            rf.fit(train_x, train_y)\n            rf_preds = rf.predict_proba(valid_x)[:, 1]\n            oof_preds_rf[valid_idx] = np.reshape(rf_preds, (rf_preds.shape[0],1))\n            \n            #LGBM\n            lgbm = LGBMClassifier(\n            nthread=4,\n            max_depth=8,\n            min_split_gain=0.0222415,\n            min_child_weight=40,\n            silent=False,\n            verbose=-1,\n            n_estimators= 1400,\n            num_leaves= 77,\n            learning_rate= 0.007641070180129345,\n            min_child_samples= 460,\n            subsample_for_bin= 240000,\n            reg_lambda= 0.2040816326530612,\n            reg_alpha= 0.8775510204081632,\n            subsample= 0.9494949494949496,\n            colsample_bytree= 0.7333333333333333\n            )\n\n            lgbm.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n                     eval_metric= 'auc', verbose= 500, early_stopping_rounds= 200)\n            lgbm_preds = lgbm.predict_proba(valid_x, num_iteration=lgbm.best_iteration_)[:,1]\n            oof_preds_lgbm[valid_idx] = np.reshape(lgbm_preds, (rf_preds.shape[0],1))\n            \n            print('>>\\t PREDICTING!')\n            temp_preds_rf = rf.predict_proba(test_features)[:,1]\n            final_preds_rf += np.reshape(temp_preds_rf, (temp_preds_rf.shape[0],1))\n            \n            temp_preds_lgbm = lgbm.predict_proba(test_features, num_iteration=lgbm.best_iteration_)[:,1]\n            final_preds_lgbm += np.reshape(temp_preds_lgbm, (temp_preds_lgbm.shape[0],1))\n            print('>>\\t PREDICTING DONE!\\n')\n        \n            del train_y, valid_y, train_x, valid_x, rf, temp_preds_rf, rf_preds, lgbm, temp_preds_lgbm, lgbm_preds\n            gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cb0f72542b728c46a109497c9a024375e32ad8d"},"cell_type":"code","source":"optimal_threshold_rf = threshold_search(train_labels, oof_preds_rf)\noptimal_threshold_lgbm = threshold_search(train_labels, oof_preds_lgbm)\nprint(optimal_threshold_rf,  '\\n', optimal_threshold_lgbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c2ddb25190e11df45bbba198df380013155548b"},"cell_type":"code","source":"optimal_threshold = threshold_search(train_labels, (oof_preds_rf+oof_preds_lgbm)/2)\nprint(optimal_threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bab49491680c6f3ea53326e3560b9559f536eca2"},"cell_type":"code","source":"final_preds = (final_preds_lgbm + final_preds_rf)/2\nfinal_preds = final_preds/num_folds\nprint('>>\\t CREATING FINAL SUBMISSION FILE!')\nfinal_preds = (final_preds > optimal_threshold['threshold']).astype(int)\nsample = pd.read_csv(PATH+'sample_submission.csv')\nsample['prediction'] = final_preds\nsample.to_csv('submission.csv', index=False)\nprint('>>\\t CREATING FINAL SUBMISSION FILE \\t DONE!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d6bdddeea8e04f740caa04c0e3e1d407e60460f"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfcda7e2b4014c15aabc21a2a7478c0b7d84188c"},"cell_type":"code","source":"np.array(np.unique(final_preds, return_counts=True)).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e58d4671474ff751c0bb5fececcfc0283229d63a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}