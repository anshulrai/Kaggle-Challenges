{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import gc\nimport re\nimport os\nimport pandas as pd\nimport numpy as np\nfrom unidecode import unidecode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nimport string\nimport re\nimport math\nimport operator\nfrom pyphen import Pyphen\nimport time\nprint(os.listdir(\"../input\"))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['train-test-result-analysis', 'quora-insincere-questions-classification']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a2b61717e468b5a6b9820240af45236260c7168"
      },
      "cell_type": "code",
      "source": "from keras.models import Model, Sequential\nfrom keras import layers\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras import backend as K\nfrom keras import optimizers",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3334066cad56e6bc24f7060499bb332183338a44",
        "_kg_hide-input": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "class CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "84105e1b614bd5f92f7007155963fba24e876f52"
      },
      "cell_type": "code",
      "source": "exclude = list(string.punctuation)\ndic = Pyphen(lang='en_US')\n\ndef legacy_round(number, points=0):\n    p = 10 ** points\n    return float(math.floor((number * p) + math.copysign(0.5, number))) / p\n\ndef char_count(text, ignore_spaces=True):\n        if ignore_spaces:\n            text = text.replace(\" \", \"\")\n        return len(text)\n\ndef lexicon_count(text):\n        count = len(text.split())\n        return count\n    \ndef syllable_count(text):\n        text = text.lower()\n        text = \"\".join(x for x in text if x not in exclude)\n        if not text:\n            return 0\n        count = 0\n        vowels = 'aeiouy'\n        for word in text.split(' '):\n            #word_hyphenated = dic.inserted(word)\n            #count += max(1, word_hyphenated.count(\"-\") + 1)\n            word = word.strip(\".:;?!\")\n            if len(word) < 1:\n                continue\n            if word[0] in vowels:\n                count +=1\n            for index in range(1,len(word)):\n                if word[index] in vowels and word[index-1] not in vowels:\n                    count +=1\n            if word.endswith('e'):\n                count -= 1\n            if word.endswith('le'):\n                count+=1\n            if count == 0:\n                count +=1\n        return count\n\ndef sentence_count(text):\n        ignore_count = 0\n        sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]*[ |\\n](?=[A-Z])', text)\n        for sentence in sentences:\n            if lexicon_count(sentence) <= 2:\n                ignore_count += 1\n        return max(1, len(sentences) - ignore_count)\n    \ndef avg_sentence_length(text):\n        try:\n            asl = float(lexicon_count(text) / sentence_count(text))\n            return legacy_round(asl, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_syllables_per_word(text):\n        syllable = syllable_count(text)\n        words = lexicon_count(text)\n        try:\n            syllables_per_word = float(syllable) / float(words)\n            return legacy_round(syllables_per_word, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef avg_letter_per_word(text):\n        try:\n            letters_per_word = float(\n                char_count(text) / lexicon_count(text))\n            return legacy_round(letters_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n\n\ndef avg_sentence_per_word(text):\n        try:\n            sentence_per_word = float(\n                sentence_count(text) / lexicon_count(text))\n            return legacy_round(sentence_per_word, 2)\n        except ZeroDivisionError:\n            return 0.0\n        \ndef flesch_reading_ease(text):\n        sentence_length = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = (\n            206.835\n            - float(1.015 * sentence_length)\n            - float(84.6 * syllables_per_word)\n        )\n        return legacy_round(flesch, 2)\n\ndef flesch_kincaid_grade(text):\n        sentence_lenth = avg_sentence_length(text)\n        syllables_per_word = avg_syllables_per_word(text)\n        flesch = (\n            float(0.39 * sentence_lenth)\n            + float(11.8 * syllables_per_word)\n            - 15.59)\n        return legacy_round(flesch, 1)\n\ndef polysyllabcount(text):\n        count = 0\n        for word in text.split():\n            wrds = syllable_count(word)\n            if wrds >= 3:\n                count += 1\n        return count\n\ndef smog_index(text):\n        sentences = sentence_count(text)\n        if sentences >= 3:\n            try:\n                poly_syllab = polysyllabcount(text)\n                smog = (\n                    (1.043 * (30 * (poly_syllab / sentences)) ** .5)\n                    + 3.1291)\n                return legacy_round(smog, 1)\n            except ZeroDivisionError:\n                return 0.0\n        else:\n            return 0.0\n\ndef coleman_liau_index(text):\n        letters = legacy_round(avg_letter_per_word(text)*100, 2)\n        sentences = legacy_round(avg_sentence_per_word(text)*100, 2)\n        coleman = float((0.058 * letters) - (0.296 * sentences) - 15.8)\n        return legacy_round(coleman, 2)\n\ndef automated_readability_index(text):\n        chrs = char_count(text)\n        words = lexicon_count(text)\n        sentences = sentence_count(text)\n        try:\n            a = float(chrs)/float(words)\n            b = float(words) / float(sentences)\n            readability = (\n                (4.71 * legacy_round(a, 2))\n                + (0.5 * legacy_round(b, 2))\n                - 21.43)\n            return legacy_round(readability, 1)\n        except ZeroDivisionError:\n            return 0.0\n\ndef linsear_write_formula(text):\n        easy_word = 0\n        difficult_word = 0\n        text_list = text.split()[:100]\n        for word in text_list:\n            if syllable_count(word) < 3:\n                easy_word += 1\n            else:\n                difficult_word += 1\n        text = ' '.join(text_list)\n        number = float(\n            (easy_word * 1 + difficult_word * 3)\n            / sentence_count(text))\n        if number <= 20:\n            number -= 2\n        return number / 2\n",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b97aa25ce6720cb9f465e9d4da1995c74a7918f"
      },
      "cell_type": "code",
      "source": "text_function_list = [char_count, lexicon_count, syllable_count, sentence_count, avg_letter_per_word,\n                     avg_sentence_length, avg_sentence_per_word, avg_syllables_per_word, flesch_kincaid_grade, flesch_reading_ease,\n                     polysyllabcount, smog_index, coleman_liau_index, automated_readability_index, linsear_write_formula]",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "639b9ea1e17020b519446c944f3a59b501c6d64a"
      },
      "cell_type": "code",
      "source": "def f1_score(true,pred): #considering sigmoid activation, threshold = 0.5\n    pred = K.cast(K.greater(pred,0.5), K.floatx())\n\n    groundPositives = K.sum(true) + K.epsilon()\n    correctPositives = K.sum(true * pred) + K.epsilon()\n    predictedPositives = K.sum(pred) + K.epsilon()\n\n    precision = correctPositives / predictedPositives\n    recall = correctPositives / groundPositives\n\n    m = (2 * precision * recall) / (precision + recall)\n\n    return m",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv('../input/quora-insincere-questions-classification/train.csv', usecols=['question_text', 'target'])\nval_df = pd.read_csv('../input/train-test-result-analysis/val_data.csv')",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73cdb32db01c5ec34c709e4cc6576726de8abf3d"
      },
      "cell_type": "code",
      "source": "val_idx = val_df['column_index'].values\nbad_df = train_df.index.isin(val_idx)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9500bc452131666d04cff1b34e17ff67bfce1425"
      },
      "cell_type": "code",
      "source": "val_df = train_df[bad_df]\ntrain_df = train_df[~bad_df]\n\nprint(train_df.shape, val_df.shape)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(1249752, 2) (56370, 2)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5974e0bcc9641289643a1381b362c08c622450d"
      },
      "cell_type": "code",
      "source": "def clean_text(x):\n    special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3be6edd9d062c7bc9821f725e49531e615888efc"
      },
      "cell_type": "code",
      "source": "train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text(str(x)))\nval_df['question_text'] = val_df['question_text'].apply(lambda x: clean_text(str(x)))",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a84b3f8117091395563ea35368c3e8bb32f5ea36"
      },
      "cell_type": "code",
      "source": "train_sentences = train_df['question_text']\ntrain_labels = train_df['target']\nval_sentences = val_df['question_text']\nval_labels = val_df['target']",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4f82c889b47596aed2403e8fe34d25e0b65282c"
      },
      "cell_type": "code",
      "source": "def add_features(df):\n    print('Processing features for dataframe!\\n')\n    df['question_text'] = df['question_text'].apply(lambda x:str(x))\n    for text_function in text_function_list:\n        start_time = time.time()\n        df[text_function.__name__] = df['question_text'].apply(lambda x: text_function(str(x)))\n        print('Processing features for function {} took {} minutes'.format(text_function.__name__, (time.time()-start_time)/60))\n    df['total_length'] = df['question_text'].apply(len)\n    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/(float(row['total_length'])+1),\n                                axis=1)\n    df['num_words'] = df['question_text'].str.count('\\S+')\n    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / (df['num_words']+1)\n    del df['num_unique_words'], df['num_words'], df['capitals'], df['total_length']\n    gc.collect()\n    print('Done!\\n')\n    return df",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72c4bc8eaa997a44b8b7aa12f310b515863309d6"
      },
      "cell_type": "code",
      "source": "start = time.time()\ntrain_df = add_features(train_df)\nprint('\\nTook {} minutes to process train\\n'.format((time.time()-start)/60))\nval_df = add_features(val_df)\nprint('\\nTook {} minutes to process val\\n'.format((time.time()-start)/60))\nprint('\\n\\nTook {} minutes total'.format((time.time()-start)/60))",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processing features for dataframe!\n\nProcessing features for function char_count took 0.029163781801859537 minutes\nProcessing features for function lexicon_count took 0.030443596839904784 minutes\nProcessing features for function syllable_count took 1.333863115310669 minutes\nProcessing features for function sentence_count took 0.12348668575286866 minutes\nProcessing features for function avg_letter_per_word took 0.07547036806742351 minutes\nProcessing features for function avg_sentence_length took 0.17072641849517822 minutes\nProcessing features for function avg_sentence_per_word took 0.16872075001398723 minutes\nProcessing features for function avg_syllables_per_word took 1.4330856919288635 minutes\nProcessing features for function flesch_kincaid_grade took 1.5839717229207357 minutes\nProcessing features for function flesch_reading_ease took 1.6255208015441895 minutes\nProcessing features for function polysyllabcount took 1.471647580464681 minutes\nProcessing features for function smog_index took 0.19350154399871827 minutes\nProcessing features for function coleman_liau_index took 0.28250720500946047 minutes\nProcessing features for function automated_readability_index took 0.23551791111628215 minutes\nProcessing features for function linsear_write_formula took 1.6366721153259278 minutes\nDone!\n\n\nTook 11.293628493944803 minutes to process train\n\nProcessing features for dataframe!\n\nProcessing features for function char_count took 0.001534263292948405 minutes\nProcessing features for function lexicon_count took 0.0016280651092529298 minutes\nProcessing features for function syllable_count took 0.06056335767110189 minutes\nProcessing features for function sentence_count took 0.005647977193196614 minutes\nProcessing features for function avg_letter_per_word took 0.003642725944519043 minutes\nProcessing features for function avg_sentence_length took 0.007803797721862793 minutes\nProcessing features for function avg_sentence_per_word took 0.007627439498901367 minutes\nProcessing features for function avg_syllables_per_word took 0.06272337436676026 minutes\nProcessing features for function flesch_kincaid_grade took 0.07271813948949178 minutes\nProcessing features for function flesch_reading_ease took 0.07253013054529826 minutes\nProcessing features for function polysyllabcount took 0.065910271803538 minutes\nProcessing features for function smog_index took 0.007603387037913005 minutes\nProcessing features for function coleman_liau_index took 0.013913253943125406 minutes\nProcessing features for function automated_readability_index took 0.011234267552693685 minutes\nProcessing features for function linsear_write_formula took 0.07390456994374593 minutes\nDone!\n\n\nTook 11.80462855497996 minutes to process val\n\n\n\nTook 11.80463077624639 minutes total\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a981dfb1ad34c317d8177eca42c6a41107bb3cf5"
      },
      "cell_type": "code",
      "source": "val_df.head()",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "                                        question_text       ...         words_vs_unique\n13  Can we use our external hard disk as a OS as w...       ...                0.809524\n19  How many baronies might exist within a county ...       ...                0.900000\n30  Which babies are more sweeter to their parents...       ...                0.812500\n36  Why my package still is ISC since May , and I ...       ...                0.933333\n76         Can we get ITC on charges levied by banks?       ...                0.900000\n\n[5 rows x 19 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question_text</th>\n      <th>target</th>\n      <th>char_count</th>\n      <th>lexicon_count</th>\n      <th>syllable_count</th>\n      <th>sentence_count</th>\n      <th>avg_letter_per_word</th>\n      <th>avg_sentence_length</th>\n      <th>avg_sentence_per_word</th>\n      <th>avg_syllables_per_word</th>\n      <th>flesch_kincaid_grade</th>\n      <th>flesch_reading_ease</th>\n      <th>polysyllabcount</th>\n      <th>smog_index</th>\n      <th>coleman_liau_index</th>\n      <th>automated_readability_index</th>\n      <th>linsear_write_formula</th>\n      <th>caps_vs_length</th>\n      <th>words_vs_unique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>Can we use our external hard disk as a OS as w...</td>\n      <td>0</td>\n      <td>77</td>\n      <td>20</td>\n      <td>26</td>\n      <td>1</td>\n      <td>3.85</td>\n      <td>20.0</td>\n      <td>0.05</td>\n      <td>1.3</td>\n      <td>7.6</td>\n      <td>76.56</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>5.05</td>\n      <td>6.7</td>\n      <td>13.00</td>\n      <td>0.030928</td>\n      <td>0.809524</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>How many baronies might exist within a county ...</td>\n      <td>0</td>\n      <td>47</td>\n      <td>9</td>\n      <td>17</td>\n      <td>1</td>\n      <td>5.22</td>\n      <td>9.0</td>\n      <td>0.11</td>\n      <td>1.9</td>\n      <td>10.3</td>\n      <td>36.96</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>11.22</td>\n      <td>7.7</td>\n      <td>5.50</td>\n      <td>0.017857</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Which babies are more sweeter to their parents...</td>\n      <td>1</td>\n      <td>72</td>\n      <td>15</td>\n      <td>20</td>\n      <td>2</td>\n      <td>4.80</td>\n      <td>7.5</td>\n      <td>0.13</td>\n      <td>1.3</td>\n      <td>2.7</td>\n      <td>89.24</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>8.19</td>\n      <td>4.9</td>\n      <td>2.75</td>\n      <td>0.022989</td>\n      <td>0.812500</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Why my package still is ISC since May , and I ...</td>\n      <td>0</td>\n      <td>51</td>\n      <td>14</td>\n      <td>16</td>\n      <td>1</td>\n      <td>3.64</td>\n      <td>14.0</td>\n      <td>0.07</td>\n      <td>1.1</td>\n      <td>2.9</td>\n      <td>99.57</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>3.24</td>\n      <td>2.7</td>\n      <td>7.00</td>\n      <td>0.092308</td>\n      <td>0.933333</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>Can we get ITC on charges levied by banks?</td>\n      <td>0</td>\n      <td>34</td>\n      <td>9</td>\n      <td>10</td>\n      <td>1</td>\n      <td>3.78</td>\n      <td>9.0</td>\n      <td>0.11</td>\n      <td>1.1</td>\n      <td>0.9</td>\n      <td>104.64</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>2.87</td>\n      <td>0.9</td>\n      <td>3.50</td>\n      <td>0.093023</td>\n      <td>0.900000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06850a2bf7c50adf4ba121ec6cf468431def71ac"
      },
      "cell_type": "code",
      "source": "train_features = train_df.drop(['question_text', 'target'], axis=1)\nval_features = val_df.drop(['question_text', 'target'], axis=1)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a74a50176462462f839195f32a98c3f6f1987d9c"
      },
      "cell_type": "code",
      "source": "ss = StandardScaler()\nss.fit(np.vstack((train_features, val_features)))\ntrain_features = ss.transform(train_features)\nval_features = ss.transform(val_features)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3754d4d288fe5411ee0cc26112a17dd34a6e427c",
        "_cell_guid": "03b49435-b06b-4f72-b8f2-27faae8cf478",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "gc.collect()",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "119"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ab720adb979acbb71bdbc1b0baf975b592049ea"
      },
      "cell_type": "code",
      "source": "max_features = 20000\nmaxlen = 100",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fafbeb9c7604d9306ee0b11831769b616bb3b49"
      },
      "cell_type": "code",
      "source": "tokenizer = text.Tokenizer(num_words=max_features)",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9ebc89a3e784567de70e9b77b514e0553e75a43"
      },
      "cell_type": "code",
      "source": "tokenizer.fit_on_texts(list(train_sentences) + list(val_sentences))",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a228c36b8fad3cf02e19cbb43210bb10735807a2"
      },
      "cell_type": "code",
      "source": "tokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62da3970827adfad76ff96f80559478c11057516"
      },
      "cell_type": "code",
      "source": "tokenized_val = tokenizer.texts_to_sequences(val_sentences)\nX_val = sequence.pad_sequences(tokenized_val, maxlen=maxlen)",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b21e0fff87623a12f7d276bb9f7e7d04f8742d6f"
      },
      "cell_type": "code",
      "source": "gc.collect()",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "553427f7b7e51c4c6133c41165a90bc554c0a279"
      },
      "cell_type": "code",
      "source": "batch_size = 1024\nepochs = 2\nembed_size = 300",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17ea35d6c1916ed1fd624de7f09c7f6ac7f77b3a"
      },
      "cell_type": "code",
      "source": "def dnn_model(features, train_flag = True, embedding_weights=None):\n    features_input = layers.Input(shape=(features.shape[1],))\n    inp = layers.Input(shape=(maxlen, ))\n    if embedding_weights is None:\n        x = layers.Embedding(max_features, embed_size, trainable=train_flag)(inp)\n    else:\n        x = layers.Embedding(max_features, embedding_weights.shape[1], weights=[embedding_weights], trainable=train_flag)(inp)\n    x = layers.Bidirectional(layers.CuDNNLSTM(64, kernel_initializer='glorot_normal', return_sequences = True))(x)\n    x, x_h, x_c = layers.Bidirectional(layers.CuDNNGRU(64, kernel_initializer='glorot_normal', return_sequences=True, return_state = True))(x)\n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool, x_h, max_pool, features_input])\n    x = layers.Dense(32, activation=\"tanh\", kernel_initializer='glorot_normal')(x)\n    x = layers.Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_normal')(x)\n    model = Model(inputs=[inp,features_input], outputs=x)\n    adam = optimizers.adam(clipvalue=1.0)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=[f1_score])\n\n    return model",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3ccca97c9211467cc412d93fa176307b20e91a5c"
      },
      "cell_type": "markdown",
      "source": "# GLOVE"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78cbcb1c59b36caeb1507e5f054ed9c471a1dc7f"
      },
      "cell_type": "code",
      "source": "weight_path=\"early_weights.hdf5\"\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\nearly_stopping = EarlyStopping(monitor=\"val_f1_score\", mode=\"max\", patience=4)\n#clr = CyclicLR(base_lr=0.0003, max_lr=0.005, step_size=2000.)\ncallbacks = [checkpoint, early_stopping]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d23cba7120d3b8c206801fa19e8754c6de44dc9"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n\ndel word_index, embeddings_index, all_embs, nb_words\ngc.collect()",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9fbb2b2b63e41f45c15ae103f8ae05b5549747c"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, False, embedding_matrix_1)",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            (None, 100)          0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 100, 300)     6000000     input_2[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 100, 128)     187392      embedding_1[0][0]                \n__________________________________________________________________________________________________\nbidirectional_2 (Bidirectional) [(None, 100, 128), ( 74496       bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nglobal_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\nglobal_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            (None, 17)           0                                            \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 337)          0           global_average_pooling1d_1[0][0] \n                                                                 bidirectional_2[0][1]            \n                                                                 global_max_pooling1d_1[0][0]     \n                                                                 input_1[0][0]                    \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 32)           10816       concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n==================================================================================================\nTotal params: 6,272,737\nTrainable params: 272,737\nNon-trainable params: 6,000,000\n__________________________________________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e26f4765c718fd8f57773cf8725487cb79a68d40"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 283s 226us/step - loss: 0.1208 - f1_score: 0.5394 - val_loss: 0.1125 - val_f1_score: 0.6064\n\nEpoch 00001: val_f1_score improved from -inf to 0.60642, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 278s 223us/step - loss: 0.1040 - f1_score: 0.6269 - val_loss: 0.1093 - val_f1_score: 0.6337\n\nEpoch 00002: val_f1_score improved from 0.60642 to 0.63370, saving model to early_weights.hdf5\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fca19a217b8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "171460b6e381182dc94f4b56e40e208646ff7928"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, True, embedding_matrix_1)",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "911482163a99324763fd36335fbdf1f5bed4ceee"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "569cb3e0b9ed8c1dbbfdac50e4af24bb6c36c87a"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 309s 247us/step - loss: 0.0986 - f1_score: 0.6527 - val_loss: 0.1042 - val_f1_score: 0.6494\n\nEpoch 00001: val_f1_score improved from 0.63370 to 0.64944, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 306s 245us/step - loss: 0.0874 - f1_score: 0.6991 - val_loss: 0.1049 - val_f1_score: 0.6485\n\nEpoch 00002: val_f1_score did not improve from 0.64944\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 34,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fca0b505390>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bdd2e92d053384cc9c1b1c5c8a2ee0c877d8f7d"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b3aaf1874dd2e6f9914b86faee6f94a35c9a9bf"
      },
      "cell_type": "code",
      "source": "val_preds_1 = model.predict([X_val, val_features], batch_size=1024, verbose=1)",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": "56370/56370 [==============================] - 5s 82us/step\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28bd08f215b2d82fadf82eb28b83816675046853"
      },
      "cell_type": "code",
      "source": "max_f1_score = 0\nmax_f1_threshold = ''\nfor thresh in np.arange(0.1, 0.901, 0.01):\n    thresh = np.round(thresh, 2)\n    f1_at_threshold = metrics.f1_score(val_labels, (val_preds_1>thresh).astype(int))\n    if f1_at_threshold > max_f1_score:\n        max_f1_score = f1_at_threshold\n        max_f1_threshold = thresh\nprint('Max threshold is {} with f1 score of {}'.format(max_f1_threshold, max_f1_score))",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Max threshold is 0.39 with f1 score of 0.672875131164743\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "969ca2826d7c6020c4910b7827d1a995c6bcb436"
      },
      "cell_type": "code",
      "source": "del model, weight_path, checkpoint, early_stopping, callbacks, max_f1_score, max_f1_threshold, embedding_matrix_1\ngc.collect()",
      "execution_count": 40,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cbc5ecd67ac6c402ef0f1534a16725fc3053a14d"
      },
      "cell_type": "markdown",
      "source": "# WIKI"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4aaa91e2f28bb908e4f798ed8f97cc5497e1794b"
      },
      "cell_type": "code",
      "source": "weight_path=\"early_weights.hdf5\"\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\nearly_stopping = EarlyStopping(monitor=\"val_f1_score\", mode=\"max\", patience=4)\n#clr = CyclicLR(base_lr=0.0003, max_lr=0.005, step_size=2000.)\ncallbacks = [checkpoint, early_stopping]",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d23cba7120d3b8c206801fa19e8754c6de44dc9"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\n        \ndel word_index, embeddings_index, all_embs, nb_words\ngc.collect()",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9fbb2b2b63e41f45c15ae103f8ae05b5549747c"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, False, embedding_matrix_2)",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e26f4765c718fd8f57773cf8725487cb79a68d40"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 283s 226us/step - loss: 0.1303 - f1_score: 0.4919 - val_loss: 0.1172 - val_f1_score: 0.5873\n\nEpoch 00001: val_f1_score improved from -inf to 0.58726, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 279s 223us/step - loss: 0.1097 - f1_score: 0.6021 - val_loss: 0.1117 - val_f1_score: 0.5816\n\nEpoch 00002: val_f1_score did not improve from 0.58726\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fcb469f9cf8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "171460b6e381182dc94f4b56e40e208646ff7928"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, True, embedding_matrix_2)",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "911482163a99324763fd36335fbdf1f5bed4ceee"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "569cb3e0b9ed8c1dbbfdac50e4af24bb6c36c87a"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 309s 247us/step - loss: 0.1060 - f1_score: 0.6177 - val_loss: 0.1076 - val_f1_score: 0.6088\n\nEpoch 00001: val_f1_score improved from 0.58726 to 0.60876, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 303s 243us/step - loss: 0.0927 - f1_score: 0.6758 - val_loss: 0.1085 - val_f1_score: 0.6169\n\nEpoch 00002: val_f1_score improved from 0.60876 to 0.61689, saving model to early_weights.hdf5\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 48,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fcb456c94e0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bdd2e92d053384cc9c1b1c5c8a2ee0c877d8f7d"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bee6fad7859f59da5437bbc9c3487fb0328e7242"
      },
      "cell_type": "code",
      "source": "val_preds_2 = model.predict([X_val, val_features], batch_size=1024, verbose=1)",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": "56370/56370 [==============================] - 5s 97us/step\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "efff71d0300c867b5903487f7cc1139ea18053fc"
      },
      "cell_type": "code",
      "source": "max_f1_score = 0\nmax_f1_threshold = ''\nfor thresh in np.arange(0.1, 0.901, 0.01):\n    thresh = np.round(thresh, 2)\n    f1_at_threshold = metrics.f1_score(val_labels, (val_preds_2>thresh).astype(int))\n    if f1_at_threshold > max_f1_score:\n        max_f1_score = f1_at_threshold\n        max_f1_threshold = thresh\nprint('Max threshold is {} with f1 score of {}'.format(max_f1_threshold, max_f1_score))",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Max threshold is 0.32 with f1 score of 0.6609453054950685\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20c4b7a20b555bc9b2ce38659a3afb6075f099e1"
      },
      "cell_type": "code",
      "source": "del model, weight_path, checkpoint, early_stopping, callbacks, max_f1_score, max_f1_threshold, embedding_matrix_2\ngc.collect()",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 52,
          "data": {
            "text/plain": "15"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a8e4011ec3214e5655de0e8edcc31c823d81e5e4"
      },
      "cell_type": "markdown",
      "source": "# Paragram"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a2ff7da081ed4f4d62f0b42dfd812b72480d324"
      },
      "cell_type": "code",
      "source": "weight_path=\"early_weights.hdf5\"\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\nearly_stopping = EarlyStopping(monitor=\"val_f1_score\", mode=\"max\", patience=4)\n#clr = CyclicLR(base_lr=0.0003, max_lr=0.005, step_size=2000.)\ncallbacks = [checkpoint, early_stopping]",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d23cba7120d3b8c206801fa19e8754c6de44dc9"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\ndel word_index, embeddings_index, all_embs, nb_words\ngc.collect()",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 54,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9fbb2b2b63e41f45c15ae103f8ae05b5549747c"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, False, embedding_matrix_3)",
      "execution_count": 55,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e26f4765c718fd8f57773cf8725487cb79a68d40"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 282s 226us/step - loss: 0.1226 - f1_score: 0.5346 - val_loss: 0.1149 - val_f1_score: 0.5732\n\nEpoch 00001: val_f1_score improved from -inf to 0.57321, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 276s 221us/step - loss: 0.1051 - f1_score: 0.6240 - val_loss: 0.1098 - val_f1_score: 0.6147\n\nEpoch 00002: val_f1_score improved from 0.57321 to 0.61473, saving model to early_weights.hdf5\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 56,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fcb4425e6d8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "171460b6e381182dc94f4b56e40e208646ff7928"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, True, embedding_matrix_3)",
      "execution_count": 57,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "911482163a99324763fd36335fbdf1f5bed4ceee"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 58,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "569cb3e0b9ed8c1dbbfdac50e4af24bb6c36c87a"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 310s 248us/step - loss: 0.0986 - f1_score: 0.6530 - val_loss: 0.1056 - val_f1_score: 0.6336\n\nEpoch 00001: val_f1_score improved from 0.61473 to 0.63365, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 304s 243us/step - loss: 0.0868 - f1_score: 0.7021 - val_loss: 0.1075 - val_f1_score: 0.6385\n\nEpoch 00002: val_f1_score improved from 0.63365 to 0.63852, saving model to early_weights.hdf5\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 59,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fcb37bb9390>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bdd2e92d053384cc9c1b1c5c8a2ee0c877d8f7d"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 60,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1455d5ae3895daaca055668f7b825b4b3f5922af"
      },
      "cell_type": "code",
      "source": "val_preds_3 = model.predict([X_val, val_features], batch_size=1024, verbose=1)",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": "56370/56370 [==============================] - 5s 94us/step\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c3f748579fadfe63aa9565ab1ff0331f4858ce02"
      },
      "cell_type": "code",
      "source": "max_f1_score = 0\nmax_f1_threshold = ''\nfor thresh in np.arange(0.1, 0.901, 0.01):\n    thresh = np.round(thresh, 2)\n    f1_at_threshold = metrics.f1_score(val_labels, (val_preds_3>thresh).astype(int))\n    if f1_at_threshold > max_f1_score:\n        max_f1_score = f1_at_threshold\n        max_f1_threshold = thresh\nprint('Max threshold is {} with f1 score of {}'.format(max_f1_threshold, max_f1_score))",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Max threshold is 0.34 with f1 score of 0.6659809719722294\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c64544d392a68828b944cefcdca55900f05697e6"
      },
      "cell_type": "code",
      "source": "del model, weight_path, checkpoint, early_stopping, callbacks, max_f1_score, max_f1_threshold, embedding_matrix_3\ngc.collect()",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 63,
          "data": {
            "text/plain": "15"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1e1fbbcf2ef1fdbb6c4679b22c64381faddd39ff"
      },
      "cell_type": "markdown",
      "source": "# Word2Vec"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2d8764f5c7f1ca8eedad2519de9033588c762d8"
      },
      "cell_type": "code",
      "source": "weight_path=\"early_weights.hdf5\"\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\nearly_stopping = EarlyStopping(monitor=\"val_f1_score\", mode=\"max\", patience=4)\n#clr = CyclicLR(base_lr=0.0003, max_lr=0.005, step_size=2000.)\ncallbacks = [checkpoint, early_stopping]",
      "execution_count": 66,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d23cba7120d3b8c206801fa19e8754c6de44dc9"
      },
      "cell_type": "code",
      "source": "from gensim.models import KeyedVectors\n\nEMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_4 = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    if word in embeddings_index:\n        embedding_vector = embeddings_index.get_vector(word)\n        embedding_matrix_4[i] = embedding_vector\n\ndel word_index, embeddings_index, nb_words\ngc.collect()",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 67,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9fbb2b2b63e41f45c15ae103f8ae05b5549747c"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, False, embedding_matrix_4)",
      "execution_count": 68,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e26f4765c718fd8f57773cf8725487cb79a68d40"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 281s 224us/step - loss: 0.1262 - f1_score: 0.5151 - val_loss: 0.1196 - val_f1_score: 0.6007\n\nEpoch 00001: val_f1_score improved from -inf to 0.60065, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 276s 220us/step - loss: 0.1093 - f1_score: 0.6030 - val_loss: 0.1138 - val_f1_score: 0.6246\n\nEpoch 00002: val_f1_score improved from 0.60065 to 0.62457, saving model to early_weights.hdf5\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 69,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fcb0a0825f8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "171460b6e381182dc94f4b56e40e208646ff7928"
      },
      "cell_type": "code",
      "source": "model = dnn_model(train_features, True, embedding_matrix_4)",
      "execution_count": 70,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "911482163a99324763fd36335fbdf1f5bed4ceee"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 71,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "569cb3e0b9ed8c1dbbfdac50e4af24bb6c36c87a"
      },
      "cell_type": "code",
      "source": "model.fit([X_train, train_features], train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_data=([X_val, val_features], val_labels), callbacks=callbacks)",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1249752 samples, validate on 56370 samples\nEpoch 1/2\n1249752/1249752 [==============================] - 308s 246us/step - loss: 0.1021 - f1_score: 0.6363 - val_loss: 0.1058 - val_f1_score: 0.6283\n\nEpoch 00001: val_f1_score improved from 0.62457 to 0.62826, saving model to early_weights.hdf5\nEpoch 2/2\n1249752/1249752 [==============================] - 303s 242us/step - loss: 0.0887 - f1_score: 0.6920 - val_loss: 0.1080 - val_f1_score: 0.6536\n\nEpoch 00002: val_f1_score improved from 0.62826 to 0.65362, saving model to early_weights.hdf5\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 72,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fcb09c2e2e8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bdd2e92d053384cc9c1b1c5c8a2ee0c877d8f7d"
      },
      "cell_type": "code",
      "source": "model.load_weights(weight_path)",
      "execution_count": 73,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75cb45d78066ebf6c362e4cc92d1bc19a748249c"
      },
      "cell_type": "code",
      "source": "val_preds_4 = model.predict([X_val, val_features], batch_size=1024, verbose=1)",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": "56370/56370 [==============================] - 5s 94us/step\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67dc7fac12af5c7c1147666b8a0761aaf16a7e9d"
      },
      "cell_type": "code",
      "source": "max_f1_score = 0\nmax_f1_threshold = ''\nfor thresh in np.arange(0.1, 0.901, 0.01):\n    thresh = np.round(thresh, 2)\n    f1_at_threshold = metrics.f1_score(val_labels, (val_preds_4>thresh).astype(int))\n    if f1_at_threshold > max_f1_score:\n        max_f1_score = f1_at_threshold\n        max_f1_threshold = thresh\nprint('Max threshold is {} with f1 score of {}'.format(max_f1_threshold, max_f1_score))",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Max threshold is 0.41 with f1 score of 0.666921508664628\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25aba91d9334ffad4815863e62c7fbacad83bf7d"
      },
      "cell_type": "code",
      "source": "del model, weight_path, checkpoint, early_stopping, callbacks, max_f1_score, max_f1_threshold, embedding_matrix_4\ngc.collect()",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 76,
          "data": {
            "text/plain": "15"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4267ca21bf66cb999cca20b664d28a1b22d1bc95"
      },
      "cell_type": "markdown",
      "source": "# Evaluate on 3!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a56c3e79d055af4d98cd1f5ae44344be3a8b0a84"
      },
      "cell_type": "code",
      "source": "final_preds_3 = 0.34*val_preds_1 + 0.33*val_preds_2 + 0.33*val_preds_3",
      "execution_count": 77,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7149cf8fd2d07c15e0fcde740a3e2637de31ecce"
      },
      "cell_type": "code",
      "source": "max_f1_score = 0\nmax_f1_threshold = ''\nfor thresh in np.arange(0.1, 0.901, 0.01):\n    thresh = np.round(thresh, 2)\n    f1_at_threshold = metrics.f1_score(val_labels, (final_preds_3>thresh).astype(int))\n    if f1_at_threshold > max_f1_score:\n        max_f1_score = f1_at_threshold\n        max_f1_threshold = thresh\nprint('Max threshold is {} with f1 score of {}'.format(max_f1_threshold, max_f1_score))",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Max threshold is 0.34 with f1 score of 0.6812451762284538\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ac91fb34080d6b7e0ffd3ebb91ad22f7588c3757"
      },
      "cell_type": "markdown",
      "source": "# Evaluate on 4!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4f77514c5d63c752bccc05b856debc3a019fb1e"
      },
      "cell_type": "code",
      "source": "final_preds_4 = 0.25*val_preds_1 + 0.25*val_preds_2 + 0.25*val_preds_3 + 0.25*val_preds_4",
      "execution_count": 79,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e30a45bb49ecec16d2205ba21faf32616f3a63ff"
      },
      "cell_type": "code",
      "source": "max_f1_score = 0\nmax_f1_threshold = ''\nfor thresh in np.arange(0.1, 0.901, 0.01):\n    thresh = np.round(thresh, 2)\n    f1_at_threshold = metrics.f1_score(val_labels, (final_preds_4>thresh).astype(int))\n    if f1_at_threshold > max_f1_score:\n        max_f1_score = f1_at_threshold\n        max_f1_threshold = thresh\nprint('Max threshold is {} with f1 score of {}'.format(max_f1_threshold, max_f1_score))",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Max threshold is 0.34 with f1 score of 0.6807594936708862\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}