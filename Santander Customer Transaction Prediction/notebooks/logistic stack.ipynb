{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = []\n",
    "for submission in os.listdir('../submissions/'):\n",
    "    model_type = submission.split('_')[0]\n",
    "    if model_type not in (\"rank\", \"ridge\", \"logisticstack\"):\n",
    "        submissions.append(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apg/all_dl/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC before stretch: 0.9017470042642681\n",
      "Validation AUC after stretch: 0.9017470042642681\n"
     ]
    }
   ],
   "source": [
    "n_val = len(pd.read_csv(\"../input/train.csv\", usecols=[\"target\"]))\n",
    "val_data = np.zeros((n_val, len(submissions)))\n",
    "column_names = []\n",
    "for i, submission in enumerate(submissions):\n",
    "    column_names.append(submission.split('.csv')[0])\n",
    "    val_data[:,i] = pd.read_csv(r\"../output/oof predictions/{}\".format(submission), \n",
    "                            usecols=[\"target\"]).values[:,0]\n",
    "oof_labels = pd.read_csv(\"../input/train.csv\", usecols=[\"target\"]).values[:,0]\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(val_data, oof_labels)\n",
    "val_preds = clf.predict_proba(val_data)[:,1]\n",
    "val_auc  = roc_auc_score(oof_labels, val_preds)\n",
    "print('Validation AUC before stretch: {}'.format(val_auc))\n",
    "val_preds = (val_preds - val_preds.min()) / (val_preds.max() - val_preds.min())\n",
    "val_auc  = roc_auc_score(oof_labels, val_preds)\n",
    "print('Validation AUC after stretch: {}'.format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16496851,  2.50557337, -0.58037078,  1.9081381 ,  0.0497083 ,\n",
       "         0.01197462,  0.20523732,  0.39422658,  0.19057639,  0.35266549,\n",
       "         0.61240815, -0.48560536, -0.24919708,  1.94169559]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = len(pd.read_csv(\"../input/test.csv\", usecols=[\"ID_code\"]))\n",
    "test_data = np.zeros((n_test, len(submissions)))\n",
    "column_names = []\n",
    "for i, submission in enumerate(submissions):\n",
    "    column_names.append(submission.split('.csv')[0])\n",
    "    test_data[:,i] = pd.read_csv(\"../submissions/{}\".format(submission), \n",
    "                               usecols=[\"target\"]).values[:,0]\n",
    "test_preds = clf.predict_proba(test_data)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.007974906977581167, 0.9114834801610247)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.min(), test_preds.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = (test_preds - test_preds.min()) / (test_preds.max() - test_preds.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.min(), test_preds.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['target'] = test_preds\n",
    "submission.to_csv('../submissions/stack.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
